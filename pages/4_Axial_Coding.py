import streamlit as st
import pandas as pd
import time
from openai import OpenAI
import json
import os
import glob
from datetime import datetime
from io import BytesIO

# =======================================================================
# 0. æ•°æ®æŒä¹…åŒ–ä¸æ¢å¤æ¨¡å— (Data Persistence)
# =======================================================================

RECOVERY_DIR = "recovery_axial_coding"

def ensure_recovery_dir():
    if not os.path.exists(RECOVERY_DIR):
        os.makedirs(RECOVERY_DIR)

def get_current_filename(topic, mode):
    """
    ç”Ÿæˆæ–‡ä»¶åï¼šä¸»é¢˜_æ¨¡å¼_æ—¥æœŸ.jsonl
    """
    safe_topic = "".join([c for c in topic if c.isalnum() or c in (' ', '_', '-')]).strip()
    if not safe_topic: safe_topic = "Untitled"
    
    safe_mode = "Auto" if "è‡ªåŠ¨" in mode else "Semi" if "åŠè‡ªåŠ¨" in mode else "Strict"
    date_str = datetime.now().strftime("%Y%m%d") 
    
    return f"{safe_topic}_{safe_mode}_{date_str}.jsonl"

def save_record_to_jsonl(record_dict, filename):
    ensure_recovery_dir()
    filepath = os.path.join(RECOVERY_DIR, filename)
    record_dict['timestamp'] = datetime.now().isoformat()
    with open(filepath, "a", encoding="utf-8") as f:
        f.write(json.dumps(record_dict, ensure_ascii=False) + "\n")

def load_from_jsonl(filepath):
    data = []
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    data.append(json.loads(line))
                except:
                    continue
    if data:
        return pd.DataFrame(data)
    else:
        return pd.DataFrame()

# =======================================================================
# 1. æ ¸å¿ƒé€»è¾‘å‡½æ•°åŒº
# =======================================================================

def call_qwen_api(api_key, model_id, messages, temperature=0.1):
    try:
        # å…¼å®¹å¤šå¹³å° API è°ƒç”¨
        if model_id in ["qwen-max", "qwen-plus", "deepseek-v3", "deepseek-r1", "kimi-k2-thinking", "glm-4.6"]:
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            client_key = api_key 
        elif model_id.startswith("gpt"):
            base_url = "https://api.openai.com/v1"
            client_key = st.session_state.get('openai_key', api_key) 
        elif model_id.startswith("gemini"):
            base_url = "https://api.gemini.com/v1" 
            client_key = st.session_state.get('gemini_key', api_key) 
        else:
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            client_key = api_key

        client = OpenAI(api_key=client_key, base_url=base_url)
        
        response = client.chat.completions.create(
            model=model_id,
            temperature=temperature,
            messages=messages,
        )
        usage = response.usage
        if usage:
            total_tokens = getattr(usage, "total_tokens", 0)
        else:
            total_tokens = 0
            
        content = response.choices[0].message.content
        if not content:
            return {"success": False, "error": "API è¿”å›äº†ç©ºå†…å®¹", "tokens": total_tokens}

        return {"success": True, "text": content, "tokens": total_tokens}
    except Exception as e:
        return {"success": False, "error": f"API Exception: {str(e)}", "tokens": 0}

def extract_json(text):
    try:
        start_index = text.find('[')
        end_index = text.rfind(']')
        if start_index != -1 and end_index != -1 and end_index > start_index:
            json_str = text[start_index : end_index + 1]
            return json.loads(json_str)
        else: return None
    except Exception:
        return None

@st.cache_data(show_spinner=False)
def to_excel_axial(axial_mapping_df, original_df=None):
    """
    å…¨é‡æ˜ å°„å¯¼å‡ºï¼šå°†è½´å¿ƒç¼–ç è§„åˆ™æ˜ å°„å›åŸå§‹æ•°æ®
    """
    output = BytesIO()
    
    if original_df is not None and not original_df.empty and not axial_mapping_df.empty:
        if 'code' in original_df.columns and 'code' in axial_mapping_df.columns:
            # å‡†å¤‡æ˜ å°„è¡¨ (å–æœ€æ–°è§„åˆ™)
            mapping_rules = axial_mapping_df.drop_duplicates(subset=['code'], keep='last')
            cols_to_use = [c for c in mapping_rules.columns if c in ['code', 'category', 'confidence', 'reasoning', 'status']]
            mapping_rules = mapping_rules[cols_to_use]
            
            # Left Join
            merged_df = pd.merge(original_df, mapping_rules, on='code', how='left')
            merged_df['category'] = merged_df['category'].fillna('å¾…å½’ç±»')
            final_df = merged_df
        else:
            final_df = axial_mapping_df
    else:
        final_df = axial_mapping_df

    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        final_df.to_excel(writer, index=False, sheet_name='Axial_Full_Data')
        if not axial_mapping_df.empty:
            axial_mapping_df.to_excel(writer, index=False, sheet_name='Coding_Rules_Only')
            
    processed_data = output.getvalue()
    return processed_data

def get_definition_prompt(domain, topic, raw_keywords):
    return f"""
ä½ æ˜¯ä¸€åèµ„æ·±çš„è´¨æ€§ç ”ç©¶ä¸“å®¶ã€‚
è¯·åŸºäºã€{domain}ã€‘é¢†åŸŸï¼Œé’ˆå¯¹ã€{topic}ã€‘è¿™ä¸€ç ”ç©¶ä¸»é¢˜ï¼Œä¸ºç”¨æˆ·æä¾›çš„ç»´åº¦å…³é”®è¯ç”Ÿæˆç®€çŸ­ã€ç²¾å‡†çš„â€œæ“ä½œæ€§å®šä¹‰â€ã€‚

ã€è¾“å…¥å…³é”®è¯ã€‘
{raw_keywords}

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. **å»é‡ä¸ç²¾ç¡®åŒ–**ï¼šæ¯ä¸ªå®šä¹‰å¿…é¡»å…·æœ‰æ’ä»–æ€§ï¼Œé¿å…ä¸åŒç»´åº¦ä¹‹é—´çš„å®šä¹‰é‡å ã€‚
2. **è¯­å¢ƒç»“åˆ**ï¼šå®šä¹‰å¿…é¡»ç´§æ‰£â€œ{topic}â€çš„ç ”ç©¶è¯­å¢ƒï¼Œè€Œéé€šç”¨çš„å­—å…¸è§£é‡Šã€‚
3. **æ ¼å¼**ï¼šç›´æ¥è¾“å‡ºåˆ—è¡¨ï¼Œæ ¼å¼ä¸ºâ€œç»´åº¦å: å®šä¹‰å†…å®¹â€ï¼Œæ— å¤šä½™æ–‡å­—ã€‚

ã€è¾“å‡ºç¤ºä¾‹ã€‘
(å‡è®¾ä¸»é¢˜æ˜¯è¿œç¨‹åŠå…¬æ•ˆç‡)
æŠ€æœ¯éšœç¢: æŒ‡å‘˜å·¥åœ¨è¿œç¨‹å·¥ä½œä¸­é‡åˆ°çš„ç½‘ç»œå»¶è¿Ÿã€è½¯ä»¶å´©æºƒæˆ–ç¡¬ä»¶æ•…éšœç­‰å…·ä½“é˜»ç¢ã€‚
æ²Ÿé€šæ–­å±‚: æŒ‡å›¢é˜Ÿæˆå‘˜å› ç¼ºä¹éè¯­è¨€çº¿ç´¢è€Œå¯¼è‡´çš„ä¿¡æ¯è¯¯è§£æˆ–åé¦ˆæ»åã€‚
    """

def generate_definitions(api_key, model_id, domain, topic, raw_keywords):
    prompt = get_definition_prompt(domain, topic, raw_keywords)
    messages = [{"role": "user", "content": prompt}]
    return call_qwen_api(api_key, model_id, messages, temperature=0.7)

def create_axial_coding_prompt(dimension_list, batch_data):
    """
    æ„å»ºç¬¦åˆæ‰æ ¹ç†è®ºé€»è¾‘çš„ Prompt
    batch_data: [{'code': '...', 'quote': '...'}] (quote å¯èƒ½æ˜¯æ‹¼æ¥åçš„å¤šæ¡)
    """
    dims_display = list(dimension_list)
    if "æ— å¯¹åº”ç»´åº¦" not in dims_display:
        dims_display.append("æ— å¯¹åº”ç»´åº¦: è¯¥ç¼–ç æ— æ³•å½’å…¥ä¸Šè¿°ä»»ä½•ç»´åº¦ï¼Œå±äºç¦»ç¾¤ç‚¹æˆ–éœ€è¦æ–°ç»´åº¦ã€‚")
    
    dims_str = "\n".join([f"- {d}" for d in dims_display])
    
    system_content = f"""
ä½ æ˜¯ä¸€ä½æ‰§è¡Œâ€œè½´å¿ƒç¼–ç ï¼ˆAxial Codingï¼‰â€çš„è´¨æ€§ç ”ç©¶åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†åº•å±‚çš„â€œå¼€æ”¾ç¼–ç â€å½’çº³åˆ°æ ¸å¿ƒç»´åº¦ä¸­ã€‚

ã€ä¸€ã€ç¼–ç æ‰‹å†Œ (Codebook)ã€‘
è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹ç»´åº¦çš„**æ“ä½œæ€§å®šä¹‰**è¿›è¡Œåˆ†ç±»ï¼Œä¸¥ç¦ä»…å‡­ç»´åº¦åç§°çŒœæµ‹ï¼š
{dims_str}

ã€äºŒã€æ“ä½œé€»è¾‘ï¼šä¸æ–­æ¯”è¾ƒæ³• (Constant Comparative Method)ã€‘
è™½ç„¶ä½ åªéœ€è¾“å‡ºç»“æœï¼Œä½†è¯·åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
1. **æƒ…å¢ƒè¿˜åŸ**ï¼šä»”ç»†é˜…è¯»å¼•æ–‡ï¼ˆQuoteï¼‰ã€‚è‹¥å¼•æ–‡åŒ…å«å¤šæ¡ï¼Œè¯·ç»¼åˆè€ƒè™‘å…¶å…±æ€§ã€‚è‹¥å¼•æ–‡ç¼ºå¤±æˆ–æ¨¡ç³Šï¼Œ**ä¸‹è°ƒç½®ä¿¡åº¦**ã€‚
2. **ç«äº‰æ€§å‡è®¾**ï¼šå¯¹äºæ¯æ¡æ•°æ®ï¼Œä¸è¦åªçœ‹å®ƒâ€œåƒâ€ä»€ä¹ˆï¼Œè¦åé—®å®ƒâ€œä¸ºä»€ä¹ˆä¸æ˜¯â€å…¶ä»–ç»´åº¦ã€‚
3. **æ’ä»–æ€§åˆ¤æ–­**ï¼šå¦‚æœä¸€æ¡æ•°æ®åŒæ—¶ç¬¦åˆä¸¤ä¸ªç»´åº¦çš„å®šä¹‰ï¼Œé€‰æ‹©**è¯­ä¹‰å¯¹åº”æ›´ç›´æ¥**çš„é‚£ä¸ªã€‚

ã€ä¸‰ã€ç½®ä¿¡åº¦è¯„åˆ†é‡è¡¨ (1-5)ã€‘
5: **ç†è®ºé¥±å’Œ**ã€‚ç¼–ç ä¸å®šä¹‰çš„å…³é”®è¯å®Œå…¨å¯¹åº”ï¼Œä¸”å¼•æ–‡è¯­å¢ƒæä¾›äº†å¼ºæœ‰åŠ›æ”¯æ’‘ã€‚
4: **é«˜åº¦åŒ¹é…**ã€‚é€»è¾‘é€šé¡ºï¼Œæ— æ˜æ˜¾æ­§ä¹‰ã€‚
3: **ä¸­åº¦åŒ¹é…**ã€‚ç¬¦åˆæ ¸å¿ƒå®šä¹‰ï¼Œä½†ç¼ºä¹è¯­å¢ƒç»†èŠ‚ï¼Œæˆ–å­˜åœ¨å¤šä¹‰æ€§ã€‚
2: **è¯æ®ä¸è¶³**ã€‚ä»…æœ‰å¾®å¼±è”ç³»ï¼Œå»ºè®®äººå·¥å¤æ ¸ã€‚
1: **æ— æ³•åˆ¤æ–­**ã€‚ä¿¡æ¯ç¼ºå¤±æˆ–å®Œå…¨ä¸ç›¸å…³ã€‚

ã€å››ã€è¾“å‡ºæ ¼å¼ã€‘
ä»…è¾“å‡º JSON æ•°ç»„ã€‚ä¸è¦è§£é‡Šï¼Œä¸è¦ Markdownã€‚

[
    {{
        "CodeName": "...",
        "AssignedCategory": "...",
        "Confidence": 5
    }}
]
"""
    data_input_str = ""
    for item in batch_data:
        c = item.get('code', 'æœªçŸ¥')
        q = item.get('quote', '')
        if not q or q == "æ— " or q == "ï¼ˆæ— å¼•ç”¨ï¼‰":
            q_str = "ï¼ˆæ— è¯­å¢ƒï¼Œä»…åŸºäºç¼–ç åˆ†æï¼‰"
        else:
            q_str = q
        data_input_str += f"- ç¼–ç : {c}\n  å¼•æ–‡: {q_str}\n\n"

    user_content = f"è¯·å¯¹ä»¥ä¸‹ {len(batch_data)} æ¡æ•°æ®è¿›è¡Œç¼–ç å½’ç±»ï¼Œç›´æ¥è¿”å› JSON æ•°ç»„ï¼š\n\n{data_input_str}"

    return [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content}
    ]

def handle_axial_acceptance(code_name, category, confidence, reasoning=""):
    # 1. æ›´æ–° Session State
    if not st.session_state.axial_codes_df.empty:
        st.session_state.axial_codes_df = st.session_state.axial_codes_df[
            st.session_state.axial_codes_df['code'] != code_name
        ]

    record_dict = {
        'code': code_name, 
        'category': category, 
        'confidence': confidence, 
        'reasoning': reasoning, 
        'status': 'Accepted' if confidence > 0 else 'Manual'
    }
    
    new_record = pd.DataFrame([record_dict])
    st.session_state.axial_codes_df = pd.concat([st.session_state.axial_codes_df, new_record], ignore_index=True)
    
    if code_name in st.session_state.codes_to_review:
        st.session_state.codes_to_review.remove(code_name)
    
    if 'ai_suggestions' in st.session_state and code_name in st.session_state.ai_suggestions:
        del st.session_state.ai_suggestions[code_name]

    # 2. è‡ªåŠ¨ä¿å­˜åˆ° JSONL
    current_topic = st.session_state.get('research_topic_input', 'Unspecified_Topic')
    current_mode = st.session_state.get('axial_mode', 'Manual')
    filename = get_current_filename(current_topic, current_mode)
    
    save_record_to_jsonl(record_dict, filename)

def clear_axial_results():
    st.session_state.axial_codes_df = pd.DataFrame(columns=['code', 'category', 'confidence', 'reasoning', 'status'])
    if 'all_unique_codes' in st.session_state:
        st.session_state.codes_to_review = st.session_state.all_unique_codes.copy()
    st.session_state.ai_suggestions = {}
    st.session_state.is_running_axial = False
    st.success("å·²æ¸…ç©ºç»“æœï¼Œå¯ä»¥é‡æ–°å¼€å§‹ã€‚")

def get_code_frequency(code_name):
    """è·å–ç¼–ç åœ¨åŸå§‹æ•°æ®ä¸­çš„å‡ºç°é¢‘ç‡"""
    if st.session_state.open_codes is not None and 'code' in st.session_state.open_codes.columns:
        return len(st.session_state.open_codes[st.session_state.open_codes['code'] == code_name])
    return 1

# [NEW] èšåˆå¼•æ–‡åŠŸèƒ½
def get_aggregated_quotes(codes_df, code_name, limit=3):
    """
    æå–æŸä¸ªç¼–ç å¯¹åº”çš„å‰ N æ¡ä¸é‡å¤å¼•æ–‡ï¼Œæ‹¼æ¥æˆå­—ç¬¦ä¸²
    """
    if codes_df is None or codes_df.empty:
        return "æ— è¯­å¢ƒ"
    
    # ç­›é€‰ç›¸å…³è¡Œ
    related = codes_df[codes_df['code'] == code_name]
    if related.empty:
        return "æ— è¯­å¢ƒ"
    
    # è·å–ä¸ä¸ºç©ºçš„ unique å¼•æ–‡
    valid_quotes = [
        str(q) for q in related['quote'].dropna().unique() 
        if str(q).strip() and str(q) not in ["æ— ", "ï¼ˆæ— å¼•ç”¨ï¼‰", "nan"]
    ]
    
    if not valid_quotes:
        return "ï¼ˆæ— è¯­å¢ƒï¼Œä»…åŸºäºç¼–ç åˆ†æï¼‰"
    
    # æˆªå–å‰ N æ¡
    selected_quotes = valid_quotes[:limit]
    
    # æ‹¼æ¥
    if len(selected_quotes) == 1:
        return selected_quotes[0]
    else:
        return " || ".join([f"{i+1}. {q}" for i, q in enumerate(selected_quotes)])

# =======================================================================
# 2. Streamlit é¡µé¢å¸ƒå±€
# =======================================================================
st.set_page_config(page_title="åŒºåŸŸ4: è½´å¿ƒç¼–ç ", layout="wide")

with st.sidebar:
    st.header("ğŸ“‚ è¿›åº¦ç®¡ç†")
    st.info("ç³»ç»Ÿä¼šè‡ªåŠ¨å°†æ‚¨çš„ç¼–ç ç»“æœä¿å­˜åˆ° `recovery_axial_coding` æ–‡ä»¶å¤¹ä¸­ã€‚")
    
    ensure_recovery_dir()
    jsonl_files = glob.glob(os.path.join(RECOVERY_DIR, "*.jsonl"))
    jsonl_files.sort(key=os.path.getmtime, reverse=True)
    
    if jsonl_files:
        st.subheader("ğŸ“¥ æ¢å¤è¿›åº¦")
        selected_file = st.selectbox("é€‰æ‹©å†å²æ–‡ä»¶", [os.path.basename(f) for f in jsonl_files], index=0)
        
        if st.button("ğŸ”„ è½½å…¥é€‰ä¸­æ–‡ä»¶"):
            filepath = os.path.join(RECOVERY_DIR, selected_file)
            loaded_df = load_from_jsonl(filepath)
            
            if not loaded_df.empty:
                if 'code' in loaded_df.columns:
                    loaded_df = loaded_df.drop_duplicates(subset=['code'], keep='last')
                    st.session_state.axial_codes_df = loaded_df
                    
                    if st.session_state.get('all_unique_codes'):
                        completed_codes = loaded_df['code'].tolist()
                        remaining = [c for c in st.session_state.all_unique_codes if c not in completed_codes]
                        st.session_state.codes_to_review = remaining
                        
                    st.success(f"æˆåŠŸæ¢å¤ {len(loaded_df)} æ¡è®°å½•ï¼")
                    st.rerun()
                else:
                    st.error("æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘ code åˆ—")
            else:
                st.warning("è¯¥æ–‡ä»¶ä¸ºç©º")
    else:
        st.caption("æš‚æ— å†å²å­˜æ¡£")

st.title("åŒºåŸŸ4: è½´å¿ƒç¼–ç  Promptç”Ÿæˆä¸æ‰§è¡ŒåŒº ğŸ§ ")

if 'open_codes' not in st.session_state: st.session_state.open_codes = None
if 'api_key' not in st.session_state: st.session_state.api_key = None
if 'openai_key' not in st.session_state: st.session_state.openai_key = "" 
if 'gemini_key' not in st.session_state: st.session_state.gemini_key = "" 
if 'selected_model' not in st.session_state: st.session_state.selected_model = 'qwen-plus' 
if 'axial_codes_df' not in st.session_state:
    st.session_state.axial_codes_df = pd.DataFrame(columns=['code', 'category', 'confidence', 'reasoning', 'status'])
if 'codes_to_review' not in st.session_state: st.session_state.codes_to_review = []
if 'ai_suggestions' not in st.session_state: st.session_state.ai_suggestions = {} 
if 'is_running_axial' not in st.session_state: st.session_state.is_running_axial = False
if 'total_token_usage' not in st.session_state: st.session_state.total_token_usage = 0
if 'dims_input_text' not in st.session_state: st.session_state.dims_input_text = "æƒ…ç»ªè¯†åˆ«\næƒ…ç»ªè°ƒèŠ‚\nç¤¾ä¼šæ”¯æŒ"
if 'research_topic_input' not in st.session_state: st.session_state.research_topic_input = "" 

# --- æ•°æ®åŠ è½½ ---
codes_df = None
if st.session_state.open_codes is not None and not st.session_state.open_codes.empty:
    codes_df = st.session_state.open_codes

if codes_df is None:
    st.warning("âš ï¸ è¯·å…ˆåœ¨ Page 2 ç”Ÿæˆå¼€æ”¾ç¼–ç ï¼Œæˆ–åœ¨æ­¤ä¸Šä¼ æ–‡ä»¶ã€‚")
    uploaded_file = st.file_uploader("ğŸ“¥ ä¸Šä¼ å¼€æ”¾ç¼–ç æ–‡ä»¶ (XLSX, CSV, JSON, JSONL)", type=["xlsx", "csv", "json", "jsonl"])
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'): codes_df = pd.read_csv(uploaded_file)
            elif uploaded_file.name.endswith('.jsonl'): codes_df = pd.read_json(uploaded_file, lines=True)
            elif uploaded_file.name.endswith('.json'):
                try: codes_df = pd.read_json(uploaded_file)
                except ValueError: uploaded_file.seek(0); codes_df = pd.read_json(uploaded_file, lines=True)
            else: codes_df = pd.read_excel(uploaded_file, engine='openpyxl')
            
            if 'code' not in codes_df.columns: st.error("é”™è¯¯ï¼šç¼ºå°‘ 'code' åˆ—"); st.stop()
            if 'quote' not in codes_df.columns: codes_df['quote'] = "ï¼ˆæ— å¼•ç”¨ï¼‰"
            st.session_state.open_codes = codes_df
            st.success(f"âœ… åŠ è½½æˆåŠŸ: {len(codes_df)} æ¡"); st.rerun()
        except Exception as e: st.error(f"è¯»å–å¤±è´¥: {e}"); st.stop()

if codes_df is None: st.stop()

all_unique_codes = codes_df['code'].unique().tolist()
st.session_state.all_unique_codes = all_unique_codes

if not st.session_state.codes_to_review and st.session_state.axial_codes_df.empty:
     st.session_state.codes_to_review = all_unique_codes.copy()
codes_to_process = st.session_state.codes_to_review

config_col, results_col = st.columns([1, 2])

# --- å·¦ä¾§ï¼šé…ç½® ---
with config_col:
    with st.container(border=True):
        st.subheader("æ­¥éª¤ 1: é…ç½®ä¸å¯åŠ¨")
        
        api_key_input = st.text_input("ğŸ”‘ DashScope Key", type="password", value=st.session_state.get('api_key', ''), label_visibility="visible")
        if api_key_input: st.session_state.api_key = api_key_input
        
        model_options = {"ğŸ‘‘ Qwen-Max": "qwen-max", "ğŸ”¥ DeepSeek-V3": "deepseek-v3", "âš–ï¸ Qwen-Plus": "qwen-plus", "ğŸš€ DeepSeek-R1": "deepseek-r1", "ğŸŒŸ GPT-4o": "gpt-4o"}
        model_keys = list(model_options.keys())
        current_key = next((k for k, v in model_options.items() if v == st.session_state.selected_model), model_keys[0])
        sel_label = st.selectbox("ğŸ§  é€‰æ‹©æ¨¡å‹", options=model_keys, index=model_keys.index(current_key))
        st.session_state.selected_model = model_options[sel_label]
        st.session_state.model_id = st.session_state.selected_model

        st.divider()
        st.markdown("#### å®šä¹‰è½´å¿ƒç»´åº¦")
        
        with st.expander("âœ¨ AI è¾…åŠ©ç”Ÿæˆå®šä¹‰ (æ¨è)", expanded=False):
            st.caption("ä¸ºäº†è®© AI ç”Ÿæˆç²¾å‡†çš„å®šä¹‰ï¼Œè¯·è¡¥å……ä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯ï¼š")
            
            col_ctx1, col_ctx2 = st.columns(2)
            input_domain = col_ctx1.text_input("1. ç ”ç©¶é¢†åŸŸ", placeholder="ä¾‹å¦‚ï¼šå‘å±•å¿ƒç†å­¦")
            input_topic = col_ctx2.text_input("2. ç ”ç©¶ä¸»é¢˜", placeholder="ä¾‹å¦‚ï¼šé’å°‘å¹´å›é€†æœŸå†²çª")
            if input_topic: st.session_state.research_topic_input = input_topic
            
            raw_dims_input = st.text_area("3. ç»´åº¦å…³é”®è¯ (ç”¨æ¢è¡Œåˆ†éš”)", 
                                         value="", 
                                         height=100, 
                                         placeholder="ä¾‹å¦‚ï¼š\næƒ…ç»ªçˆ†å‘\nå†·å¤„ç†",
                                         key="helper_dims_input")
            
            col_h1, col_h2 = st.columns([1, 1])
            with col_h1:
                if st.button("ğŸª„ ç”Ÿæˆå¹¶å¡«å……", type="primary"):
                    if not input_domain.strip() or not input_topic.strip() or not raw_dims_input.strip():
                        st.warning("è¯·å®Œæ•´å¡«å†™ã€ç ”ç©¶é¢†åŸŸã€‘ã€ã€ç ”ç©¶ä¸»é¢˜ã€‘å’Œã€ç»´åº¦å…³é”®è¯ã€‘ï¼Œè¿™å†³å®šäº†å®šä¹‰çš„å‡†ç¡®æ€§ã€‚")
                    elif not st.session_state.get('api_key'):
                        st.error("è¯·å…ˆè¾“å…¥ API Key")
                    else:
                        with st.spinner("æ­£åœ¨åŸºäºç‰¹å®šè¯­å¢ƒç”Ÿæˆå®šä¹‰..."):
                            gen_res = generate_definitions(
                                st.session_state.api_key, 
                                st.session_state.model_id, 
                                input_domain, 
                                input_topic, 
                                raw_dims_input
                            )
                            if gen_res["success"]:
                                st.session_state['dims_input_area'] = gen_res["text"]
                                st.session_state.dims_input_text = gen_res["text"]
                                st.session_state.total_token_usage += gen_res["tokens"]
                                st.success(f"å®šä¹‰å·²ç”Ÿæˆï¼(æ¶ˆè€— {gen_res['tokens']} tokens)")
                                time.sleep(1)
                                st.rerun()
                            else:
                                st.error(gen_res["error"])
            
            with col_h2:
                if st.button("ğŸ“‹ æŸ¥çœ‹ Prompt (ç½‘é¡µç«¯ç”¨)"):
                    d_val = input_domain if input_domain else "[ç ”ç©¶é¢†åŸŸ]"
                    t_val = input_topic if input_topic else "[ç ”ç©¶ä¸»é¢˜]"
                    k_val = raw_dims_input if raw_dims_input else "[ç»´åº¦å…³é”®è¯]"
                    prompt_text = get_definition_prompt(d_val, t_val, k_val)
                    st.code(prompt_text, language="markdown")

        dimensions_input = st.text_area(
            "ç»´åº¦åˆ—è¡¨ (æ ¼å¼ï¼šç»´åº¦å: å®šä¹‰)", 
            value=st.session_state.dims_input_text, 
            height=200,
            key="dims_input_area",
            help="AI ä¼šæ ¹æ®è¿™é‡Œçš„å®šä¹‰è¿›è¡ŒåŒ¹é…ã€‚å¯ä»¥æ‰‹åŠ¨è¾“å…¥ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä¸Šæ–¹çš„è¾…åŠ©ç”Ÿæˆã€‚"
        )
        st.session_state.dims_input_text = dimensions_input

        dimension_list = [line.split(":")[0].strip() for line in dimensions_input.splitlines() if line.strip()]
        if 'æ— å¯¹åº”ç»´åº¦' not in dimension_list: dimension_list.append('æ— å¯¹åº”ç»´åº¦')
        
        st.divider()
        st.markdown("#### æ‰§è¡Œæ§åˆ¶")
        mode = st.radio("æ¨¡å¼", ["ğŸ”¹ è‡ªåŠ¨æ¨¡å¼", "ğŸ”¸ åŠè‡ªåŠ¨æ¨¡å¼", "ğŸ”º ä¸¥æ ¼æ¨¡å¼"], index=1)
        st.session_state.axial_mode = mode 
        
        batch_size = st.number_input("æ¯æ‰¹å‘é€æ¡æ•°", 1, 100, 10)

        col_btn1, col_btn2, col_btn3 = st.columns(3)
        with col_btn1:
            if st.button("ğŸŸ¢ ç»§ç»­/å¼€å§‹", type="primary"):
                if not st.session_state.get('api_key'): st.error("æ—  Key"); st.stop()
                st.session_state.is_running_axial = True
                st.rerun()
        with col_btn2:
            if st.button("â¸ï¸ æš‚åœ"):
                st.session_state.is_running_axial = False
                st.rerun()
        with col_btn3:
            if st.button("ğŸ—‘ï¸ æ¸…ç©º"):
                clear_axial_results()
                st.rerun()
        
        if st.button("ğŸ§ª æµ‹è¯•è¿è¡Œ (3æ¡)"):
             if not st.session_state.get('api_key'): st.error("æ—  Key"); st.stop()
             with st.spinner("æµ‹è¯•ä¸­..."):
                 test_codes = codes_to_process[:3]
                 test_batch_data = []
                 for c in test_codes:
                     # [MODIFIED] ä½¿ç”¨èšåˆå¼•æ–‡
                     q = get_aggregated_quotes(codes_df, c)
                     test_batch_data.append({'code': c, 'quote': q})

                 messages = create_axial_coding_prompt(dimension_list, test_batch_data)
                 res = call_qwen_api(st.session_state.api_key, st.session_state.model_id, messages)
                 if res["success"]:
                     st.session_state.total_token_usage += res["tokens"]
                     st.info(f"æµ‹è¯•è¿è¡ŒæˆåŠŸ (æ¶ˆè€— {res['tokens']} tokens)")
                     parsed = extract_json(res["text"])
                     if parsed:
                         st.json(parsed)
                     else:
                         st.error("JSON è§£æå¤±è´¥ï¼ŒåŸå§‹è¿”å›å¦‚ä¸‹ï¼š")
                         st.code(res["text"])
                 else: st.error(res["error"])

    with st.expander("ğŸ“‚ æŸ¥çœ‹/ä¿®æ”¹ å¼€æ”¾ç¼–ç æºæ•°æ®"):
        edited_open_codes = st.data_editor(st.session_state.open_codes, num_rows="dynamic", key="open_codes_manager", height=300)
        st.session_state.open_codes = edited_open_codes

# --- å³ä¾§ï¼šç»“æœå®¡æŸ¥å° ---
with results_col:
    
    st.markdown("### ğŸ“Š è¿›åº¦çœ‹æ¿")
    total_num = len(st.session_state.all_unique_codes)
    done_num = len(st.session_state.axial_codes_df)
    ready_num = len([c for c in st.session_state.codes_to_review if c in st.session_state.ai_suggestions])
    
    m1, m2, m3, m4 = st.columns(4)
    m1.metric("æ€»æ•°", total_num)
    m2.metric("âœ… å·²å®Œæˆ", done_num)
    m3.metric("ğŸ¤– å¾…å®¡æŸ¥", ready_num)
    m4.metric("ğŸ’° Token", st.session_state.total_token_usage) 
    
    if total_num > 0: 
        progress_val = min(done_num / total_num, 1.0)
        st.progress(progress_val)
    
    st.divider()

    if st.session_state.axial_mode == "ğŸ”¹ è‡ªåŠ¨æ¨¡å¼":
        st.subheader(f"è‡ªåŠ¨å½’ç±»ç»“æœ (å·²å½’ç±»: {len(st.session_state.axial_codes_df)})")
        if not st.session_state.axial_codes_df.empty:
            edited_df = st.data_editor(
                st.session_state.axial_codes_df,
                column_config={"category": st.column_config.SelectboxColumn("ç»´åº¦", options=dimension_list, required=True)},
                disabled=["code", "reasoning"], num_rows="dynamic", key="auto_editor", height=400
            )
            st.session_state.axial_codes_df = edited_df
        else:
            st.info("ç‚¹å‡»â€œğŸŸ¢ å¼€å§‹â€è¿›è¡Œè‡ªåŠ¨å½’ç±»ã€‚")
    else:
        st.subheader(f"å¾…å®¡æŸ¥ (å‰©ä½™ {len(st.session_state.codes_to_review)} æ¡)")
        
        if mode == "ğŸ”¸ åŠè‡ªåŠ¨æ¨¡å¼":
            ready_to_show = [c for c in st.session_state.codes_to_review if c in st.session_state.ai_suggestions]
        else:
            ready_to_show = st.session_state.codes_to_review

        if ready_to_show:
            MAX_DISPLAY = 6 
            codes_batch_disp = ready_to_show[:MAX_DISPLAY]
            cols = st.columns(2)
            
            for i, code_name in enumerate(codes_batch_disp):
                # UI ä¸Šåªæ˜¾ç¤ºç¬¬ä¸€æ¡ä½œä¸ºé¢„è§ˆï¼Œä½† AI çœ‹åˆ°äº†èšåˆçš„
                quotes = codes_df[codes_df['code'] == code_name]['quote'].tolist()
                quote_preview = quotes[0] if quotes else "æ— è¯­å¢ƒ"
                
                freq = get_code_frequency(code_name)
                
                suggestion = st.session_state.ai_suggestions.get(code_name, {})
                assigned_category = suggestion.get("category", "æ— å¯¹åº”ç»´åº¦")
                confidence = suggestion.get("confidence", 0) 
                
                is_ai = (mode == "ğŸ”¸ åŠè‡ªåŠ¨æ¨¡å¼" and assigned_category in dimension_list)
                
                with cols[i % 2]:
                    with st.container(border=True):
                        st.markdown(f"### ğŸ·ï¸ {code_name} `x{freq}`")
                        st.caption(f"å¼•æ–‡: {quote_preview}")
                        st.divider()
                        
                        act_l, act_r = st.columns([1, 1])
                        with act_l:
                            if is_ai:
                                try:
                                    score_val = int(confidence)
                                except: score_val = 0
                                
                                score_val = max(0, min(5, score_val))
                                
                                full_s = score_val
                                empty_s = 5 - full_s
                                star_html = f"<span style='color: #FFC107; font-size: 1.2em;'>{'â˜…' * full_s}</span><span style='color: #E0E0E0; font-size: 1.2em;'>{'â˜…' * empty_s}</span>"
                                st.markdown(f"<span style='font-size:0.8em; color:gray'>AI ç½®ä¿¡åº¦:</span> {star_html} <span style='font-size:0.9em'>{score_val}/5</span>", unsafe_allow_html=True)

                                st.markdown(f"**{assigned_category}**") 
                                
                                st.button("âœ… æ¥å—", key=f"acc_{code_name}", type="primary",
                                          on_click=handle_axial_acceptance,
                                          args=(code_name, assigned_category, score_val, ""))
                            else: st.markdown("*(æ— å»ºè®®)*")
                        
                        with act_r:
                            try: default_idx = dimension_list.index(assigned_category) if is_ai else 0
                            except: default_idx = len(dimension_list) - 1 
                            manual_cat = st.selectbox("äººå·¥å½’ç±»", dimension_list, key=f"man_{code_name}", label_visibility="collapsed", index=default_idx)
                            st.button("â¬‡ï¸ ç¡®è®¤", key=f"man_btn_{code_name}",
                                      on_click=handle_axial_acceptance,
                                      args=(code_name, manual_cat, 5, "äººå·¥")) 

            if len(ready_to_show) > MAX_DISPLAY:
                st.info("ç‚¹å‡»ä»»æ„æŒ‰é’®åŠ è½½ä¸‹ä¸€æ‰¹...")
            
            if mode == "ğŸ”¸ åŠè‡ªåŠ¨æ¨¡å¼" and st.session_state.is_running_axial:
                 st.caption("ğŸ”„ åå°æ­£åœ¨æŒç»­ç”Ÿæˆå»ºè®®ä¸­...")
                 
        elif not st.session_state.is_running_axial and mode == "ğŸ”¸ åŠè‡ªåŠ¨æ¨¡å¼" and st.session_state.codes_to_review:
             st.info("æš‚æ— AIå»ºè®®ã€‚è¯·ç‚¹å‡»â€œğŸŸ¢ ç»§ç»­/å¼€å§‹â€è®©AIç”Ÿæˆå»ºè®®ã€‚")

    if not st.session_state.codes_to_review:
        st.success("ğŸ‰ æ‰€æœ‰å¾…å®¡æŸ¥ä»£ç å·²å¤„ç†å®Œæ¯•ï¼")

    st.divider()
    st.subheader("æ­¥éª¤ 3: ç»“æœå¯¼å‡º")
    if not st.session_state.axial_codes_df.empty:
        st.dataframe(st.session_state.axial_codes_df)
        
        export_data = to_excel_axial(st.session_state.axial_codes_df, st.session_state.open_codes)
        
        # [MODIFIED] åŠ¨æ€æ–‡ä»¶å
        cur_topic = st.session_state.get('research_topic_input', 'Research')
        safe_topic = "".join([c for c in cur_topic if c.isalnum() or c in (' ', '_', '-')]).strip()
        if not safe_topic: safe_topic = "Axial_Result"
        date_str = datetime.now().strftime("%Y%m%d_%H%M")
        file_name = f"{safe_topic}_{date_str}.xlsx"
        
        st.download_button("ğŸ’¾ å¯¼å‡ºç»“æœ (å«åŸå§‹è¡Œ)", data=export_data, file_name=file_name)

# --- æ ¸å¿ƒå¤„ç†é€»è¾‘ ---
if st.session_state.is_running_axial:
    pending_ai_codes = [c for c in st.session_state.codes_to_review if c not in st.session_state.ai_suggestions]
    
    if not pending_ai_codes and not st.session_state.codes_to_review:
        st.session_state.is_running_axial = False
        st.rerun()
    
    elif pending_ai_codes:
        if mode != "ğŸ”º ä¸¥æ ¼æ¨¡å¼":
            batch_codes = pending_ai_codes[:batch_size]
            batch_data = []
            for c in batch_codes:
                # [MODIFIED] ä½¿ç”¨èšåˆå¼•æ–‡
                q = get_aggregated_quotes(codes_df, c)
                batch_data.append({'code': c, 'quote': q})
            
            with results_col:
                with st.spinner(f"ğŸ¤– æ­£åœ¨åå°åˆ†æ {len(batch_codes)} æ¡æ•°æ®..."):
                    messages = create_axial_coding_prompt(dimension_list, batch_data)
                    res = call_qwen_api(st.session_state.api_key, st.session_state.model_id, messages)
                    
                    if res["success"]:
                        st.session_state.total_token_usage += res["tokens"]
                        results = extract_json(res["text"])
                        if isinstance(results, list):
                            for item in results:
                                c_name = item.get("CodeName")
                                category = item.get("AssignedCategory")
                                confidence = item.get("Confidence", 0) 
                                
                                st.session_state.ai_suggestions[c_name] = {
                                    "category": category,
                                    "confidence": confidence,
                                    "reasoning": ""
                                }
                                
                                if mode == "ğŸ”¹ è‡ªåŠ¨æ¨¡å¼":
                                    handle_axial_acceptance(c_name, category, confidence, "")
                                    
                            st.rerun()
                        else:
                            st.error("âš ï¸ AI è¿”å›æ•°æ®æ ¼å¼é”™è¯¯ï¼Œè§£æå¤±è´¥ã€‚è¯·æŸ¥çœ‹ä¸‹æ–¹åŸå§‹è¿”å›ã€‚")
                            with st.expander("ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹ AI åŸå§‹è¿”å›", expanded=True):
                                st.code(res["text"])
                            st.session_state.is_running_axial = False
                    else:
                        st.error(f"API Error: {res['error']}")
                        st.session_state.is_running_axial = False
        else:
            st.session_state.is_running_axial = False
            st.rerun()
