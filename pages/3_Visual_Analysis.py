import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI
import json
import time
import random
import os
import glob
from collections import Counter
from itertools import combinations
from difflib import SequenceMatcher
import platform
import datetime
import copy

# å°è¯•å¯¼å…¥æ‹–æ‹½åº“
try:
    from streamlit_sortables import sort_items
    HAS_SORTABLE = True
except ImportError:
    HAS_SORTABLE = False

# =======================================================================
# 0. æ ¸å¿ƒå·¥å…·å‡½æ•° & æ•°æ®åŠ è½½æ¨¡å—
# =======================================================================

RECOVERY_DIR = "recovery_data_visual_analysis"

def ensure_dir(dir_path):
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

def ensure_dir(dir_path):
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

def ensure_recovery_dir():
    ensure_dir(RECOVERY_DIR)

def load_from_jsonl(filepath):
    records = []
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    if line.strip(): records.append(json.loads(line))
                except: continue
    
    flat_codes = []
    for r in records:
        idx = r.get('original_row_index')
        codes_list = r.get('generated_codes', [])
        source_file = r.get('source_file', 'unknown')
        if isinstance(codes_list, list):
            for c in codes_list:
                if isinstance(c, dict):
                    flat_codes.append({
                        'source_file': source_file,
                        'original_row_index': idx,
                        # å®Œæ•´ä¿ç•™å››åˆ—çŠ¶æ€
                        'original_code': c.get('original_code', c.get('code')),
                        'peer_code': c.get('peer_code', None),
                        'aligned_code': c.get('aligned_code', c.get('code')),
                        'code': c.get('code'), # æœ€ç»ˆåˆ—
                        'quote': c.get('quote'),
                        'confidence': c.get('confidence', 0)
                    })
    return pd.DataFrame(flat_codes)

def save_current_progress(df):
    """ä¿å­˜å…¨é‡çŠ¶æ€"""
    if df.empty: return None
    ensure_recovery_dir()
    date_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"VisualAnalysis_Full_{date_str}.jsonl"
    filepath = os.path.join(RECOVERY_DIR, filename)
    
    # ç¡®ä¿ open_codes ç»“æ„æ˜¯æ ‡å‡†çš„4åˆ—
    temp_df = df.copy()
    if 'original_row_index' not in temp_df.columns: temp_df['original_row_index'] = range(len(temp_df))
    
    if 'original_row_index' in temp_df.columns:
        grouped = temp_df.groupby('original_row_index')
        with open(filepath, "w", encoding="utf-8") as f:
            for idx, group in grouped:
                first_row = group.iloc[0]
                codes_list = []
                for _, row in group.iterrows():
                    codes_list.append({
                        "original_code": row.get('original_code'),
                        "peer_code": row.get('peer_code'),
                        "aligned_code": row.get('aligned_code'),
                        "code": row['code'], 
                        "quote": row['quote'],
                        "confidence": row.get('confidence', 0)
                    })
                record = {
                    "original_row_index": int(idx) if pd.notna(idx) else None,
                    "source_file": first_row.get('source_file', 'unknown'),
                    "generated_codes": codes_list,
                    "timestamp": datetime.datetime.now().isoformat()
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        return filename
    return None

def save_analysis_progress(analysis_df, sortable_items):
    """ä¿å­˜å½“å‰çš„ç§¯æœ¨å½’ç±»çŠ¶æ€ (analysis_df å’Œ sortable_items)"""
    ensure_dir(ANALYSIS_STATE_DIR)
    date_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"AxialAnalysisState_{date_str}.json"
    filepath = os.path.join(ANALYSIS_STATE_DIR, filename)

    # 1. Prepare analysis_df (drop large embeddings)
    df_to_save = analysis_df.drop(columns=['embedding'], errors='ignore').to_dict('records')
    
    # 2. Combine all state data
    state_data = {
        'analysis_df_records': df_to_save,
        'sortable_items': sortable_items,
        'timestamp': datetime.datetime.now().isoformat()
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(state_data, f, ensure_ascii=False, indent=4)
        
    return filename

def process_analysis_state_data(state_data):
    """æ ¸å¿ƒé€»è¾‘ï¼šä»è§£æåçš„ JSON å­—å…¸ä¸­æ¢å¤ session state"""
    try:
        df_loaded = pd.DataFrame(state_data.get('analysis_df_records', []))
        if df_loaded.empty:
             st.error("åŠ è½½çš„åˆ†ææ•°æ®ä¸ºç©ºã€‚")
             return False

        # æ ¸å¿ƒæ¢å¤æ­¥éª¤
        st.session_state.analysis_df = df_loaded
        st.session_state.sortable_items = state_data.get('sortable_items', [])
        st.session_state.clusters_cache = True # æ ‡è®°ä¸ºå·²è½½å…¥

        # ç¡®ä¿ embedding åˆ—å­˜åœ¨
        if 'embedding' not in st.session_state.analysis_df.columns:
             st.session_state.analysis_df['embedding'] = None
        
        return True
    except Exception as e:
        st.error(f"æ¢å¤çŠ¶æ€å¤±è´¥: {e}")
        return False

def load_analysis_progress_from_file(filename):
    """è½½å…¥å†å²ç§¯æœ¨å½’ç±»çŠ¶æ€ (ä»æœ¬åœ°æ–‡ä»¶)"""
    filepath = os.path.join(ANALYSIS_STATE_DIR, filename)
    if not os.path.exists(filepath):
        st.error(f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨ã€‚")
        return False
        
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            state_data = json.load(f)
            
        return process_analysis_state_data(state_data)

    except Exception as e:
        st.error(f"è½½å…¥å¤±è´¥: {e}")
        return False

def load_analysis_progress_from_uploaded_file(uploaded_file):
    """è½½å…¥å†å²ç§¯æœ¨å½’ç±»çŠ¶æ€ (ä»ä¸Šä¼ æ–‡ä»¶ï¼Œæ”¯æŒ JSON/Excel/CSV)"""
    file_name = uploaded_file.name
    
    try:
        if file_name.endswith('.json'):
            file_content = uploaded_file.read().decode("utf-8")
            state_data = json.loads(file_content)
            
            if process_analysis_state_data(state_data):
                st.toast("JSONçŠ¶æ€æ–‡ä»¶è½½å…¥æˆåŠŸï¼")
                return True
            return False

        elif file_name.endswith(('.xlsx', '.xls', '.csv')):
            
            if file_name.endswith('.csv'):
                df_import = pd.read_csv(uploaded_file)
            else:
                df_import = pd.read_excel(uploaded_file)
                
            required_cols = ['code', 'final_category'] 
            if not all(col in df_import.columns for col in required_cols):
                st.error(f"å¯¼å…¥çš„ Excel/CSV æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—ã€‚è¯·ç¡®ä¿æ–‡ä»¶åŒ…å«ä»¥ä¸‹åˆ—: {required_cols}")
                return False

            if 'analysis_df' not in st.session_state or st.session_state.analysis_df.empty:
                st.error("è¯·å…ˆç‚¹å‡»ã€åˆå§‹åŒ–/é‡ç½® åˆ†ææ•°æ®ã€‘æŒ‰é’®ï¼Œè·å– AI èšç±»ç»“æœåå†å¯¼å…¥ Excel/CSV æ–‡ä»¶è¿›è¡Œåˆ†ç±»è¦†ç›–ã€‚")
                return False
            
            # æ ¸å¿ƒè¦†ç›–é€»è¾‘
            current_df = st.session_state.analysis_df.copy()
            
            # ç¡®ä¿ code åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹è¿›è¡Œæ¯”å¯¹
            df_import['code'] = df_import['code'].astype(str) 
            
            # åˆ›å»ºä¸€ä¸ªç”¨äºæ˜ å°„çš„ Series
            update_map = df_import.set_index('code')['final_category']
            
            # ä½¿ç”¨ update_map æ›´æ–° analysis_df
            st.session_state.analysis_df['final_category'] = st.session_state.analysis_df['code'].apply(
                lambda x: update_map.get(x) if x in update_map.index else (current_df.loc[current_df['code'] == x, 'final_category'].iloc[0] if (current_df['code'] == x).any() else None)
            )
            
            st.session_state.analysis_df.fillna({'final_category': None}, inplace=True)
            
            st.toast("Excel/CSV åˆ†ç±»ç»“æœå·²æˆåŠŸè¦†ç›–ï¼")
            return True

        else:
            st.error("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚")
            return False

    except Exception as e:
        st.error(f"å¤„ç†ä¸Šä¼ æ–‡ä»¶å¤±è´¥: é”™è¯¯ä¿¡æ¯: {e}")
        return False


# [NEW] æ’¤é”€ç³»ç»Ÿ (Undo System)
def push_history(action_name="Unknown Action"):
    """åœ¨ä¿®æ”¹æ•°æ®å‰è°ƒç”¨ï¼Œä¿å­˜å½“å‰çŠ¶æ€å¿«ç…§"""
    if 'history_stack' not in st.session_state:
        st.session_state.history_stack = []
    
    # é™åˆ¶æ ˆæ·±åº¦ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸ (å­˜æœ€è¿‘ 10 æ­¥)
    if len(st.session_state.history_stack) >= 10:
        st.session_state.history_stack.pop(0)
    
    snapshot = {
        'open_codes': st.session_state.open_codes.copy(deep=True),
        'axial_codes_df': st.session_state.axial_codes_df.copy(deep=True),
        'desc': action_name,
        'time': time.strftime("%H:%M:%S")
    }
    st.session_state.history_stack.append(snapshot)

def perform_undo():
    """æ‰§è¡Œæ’¤é”€"""
    if 'history_stack' in st.session_state and st.session_state.history_stack:
        last_state = st.session_state.history_stack.pop()
        st.session_state.open_codes = last_state['open_codes']
        st.session_state.axial_codes_df = last_state['axial_codes_df']
        st.toast(f"å·²æ’¤é”€: {last_state['desc']}")
        time.sleep(0.5)
        st.rerun()
    else:
        st.warning("æ²¡æœ‰å¯æ’¤é”€çš„æ“ä½œ")

@st.cache_data(show_spinner=False)
def get_embeddings_dashscope(text_list, api_key):
    if not text_list: return []
    client = OpenAI(
        api_key=api_key, 
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    all_embeddings = []
    batch_size = 10
    for i in range(0, len(text_list), batch_size):
        batch = text_list[i:i+batch_size]
        try:
            resp = client.embeddings.create(
                model="text-embedding-v2", 
                input=batch
            )
            batch_emb = [d.embedding for d in resp.data]
            all_embeddings.extend(batch_emb)
        except Exception as e:
            st.error(f"Embedding API Error: {e}")
            return []
    return np.array(all_embeddings)

def perform_clustering(codes, embeddings, n_clusters=None, distance_threshold=0.6):
    if len(codes) < 2: return {0: codes}
    clustering = AgglomerativeClustering(
        n_clusters=None, 
        distance_threshold=distance_threshold, 
        metric='cosine', 
        linkage='average'
    )
    labels = clustering.fit_predict(embeddings)
    clusters = {}
    for code, label in zip(codes, labels):
        if label not in clusters: clusters[label] = []
        clusters[label].append(code)
    return clusters

def find_synonym_groups(codes, embeddings, threshold=0.85):
    if len(codes) < 2: return {}
    clustering = AgglomerativeClustering(
        n_clusters=None,
        distance_threshold=1-threshold,
        metric='cosine',
        linkage='average'
    )
    labels = clustering.fit_predict(embeddings)
    groups = {}
    for i, (code, label) in enumerate(zip(codes, labels)):
        if label not in groups: groups[label] = {"codes": [], "indices": []}
        groups[label]["codes"].append(code)
        groups[label]["indices"].append(i)
    result_groups = {}
    for lbl, data in groups.items():
        if len(data["codes"]) > 1:
            if len(data["indices"]) > 1:
                group_emb = embeddings[data["indices"]]
                sim_matrix = cosine_similarity(group_emb)
                avg_sim = np.mean(sim_matrix[np.triu_indices(len(sim_matrix), k=1)])
            else: avg_sim = 1.0
            result_groups[lbl] = {"codes": data["codes"], "score": avg_sim}
    return result_groups

# è¯äº‘å›¾ï¼ˆé¢œè‰²æ·±æµ…è¡¨ç¤ºé¢‘æ¬¡ï¼‰
def generate_html_tag_cloud_color_coded(df):
    """ç”Ÿæˆä¸€ä¸ªé¢œè‰²æ·±æµ…è¡¨ç¤ºé¢‘æ¬¡çš„è¯äº‘å›¾ï¼ˆé¢‘æ¬¡è¶Šé«˜ï¼Œé¢œè‰²è¶Šæ·±ï¼‰ã€‚"""
    if df.empty or 'code' not in df.columns: return "æ— æ•°æ®"
    counts = df['code'].value_counts()
    if counts.empty: return "æ— æœ‰æ•ˆæ ‡ç­¾"
    max_count = counts.max()
    min_count = counts.min()
    tags_html = ""
    
    # åŸºç¡€é¢œè‰² (ä¾‹å¦‚è“è‰²è°ƒ)
    base_color_start = np.array([195, 232, 255]) # æµ…è“
    base_color_end = np.array([0, 58, 140]) # æ·±è“
    
    # ç»Ÿä¸€å¤§å°ï¼Œé¢œè‰²æŒ‰é¢‘æ¬¡å˜åŒ–
    size = 18 
    
    for code, count in counts.items():
        # æ ¹æ®é¢‘æ¬¡è®¡ç®—æ’å€¼æ¯”ä¾‹ (0 åˆ° 1)
        if max_count == min_count:
            ratio = 0.5
        else:
            ratio = (count - min_count) / (max_count - min_count)
            
        # çº¿æ€§æ’å€¼è®¡ç®—é¢œè‰²
        r = int(base_color_start[0] + (base_color_end[0] - base_color_start[0]) * ratio)
        g = int(base_color_start[1] + (base_color_end[1] - base_color_start[1]) * ratio)
        b = int(base_color_start[2] + (base_color_end[2] - base_color_start[2]) * ratio)
        
        color = f"#{r:02x}{g:02x}{b:02x}"
        
        tags_html += f"""<span style="font-size: {size}px; color: {color}; margin: 5px; padding: 5px; 
            display: inline-block; border: 1px solid #ccc; border-radius: 5px; font-weight: bold; background-color: #f0f8ff;"
            title="å‡ºç°é¢‘æ¬¡: {count}">{code}</span>"""
            
    return f"<div style='line-height: 2.0; text-align: center; padding: 20px; background: white; border-radius: 10px; border: 1px solid #eee; max-height: 250px; overflow-y: auto;'>{tags_html}</div>"

def reset_analysis_state():
    # ç§»é™¤ analysis_df ä¼šå¯¼è‡´ä¸‹æ¬¡éœ€è¦é‡æ–°è·‘èšç±»
    keys = ['embeddings', 'clusters_cache', 'sortable_items', 'merge_groups', 'alignment_results', 'page_num_align', 'analysis_df']
    for k in keys:
        if k in st.session_state: del st.session_state[k]

# ä¼˜åŒ–åçš„å¯¹é½ç®—æ³• (åŠ é€Ÿç‰ˆ) - ä¿æŒä¸å˜
def align_records_by_quote(df_mine, df_theirs, match_threshold=0.6):
    theirs_records = df_theirs.to_dict('records')
    alignment = []
    mine_records = df_mine.to_dict('records')
    
    # é¢„å¤„ç†ï¼šæ„å»ºç”±å¼•æ–‡é•¿åº¦ç´¢å¼•çš„åˆ—è¡¨ï¼Œå‡å°‘éå†èŒƒå›´
    theirs_buckets = {}
    for r in theirs_records:
        q_len = len(str(r.get('quote', '')))
        bucket_id = q_len // 10
        if bucket_id not in theirs_buckets: theirs_buckets[bucket_id] = []
        theirs_buckets[bucket_id].append(r)
    
    for my_row in mine_records:
        my_quote = str(my_row.get('quote', ''))
        my_len = len(my_quote)
        my_bucket = my_len // 10
        my_code = str(my_row.get('code', ''))
        
        best_match = None
        best_ratio = 0
        
        candidates = []
        # ä¼˜åŒ–ï¼šåªæ£€æŸ¥é•¿åº¦ç›¸è¿‘çš„å¼•æ–‡
        for b in [my_bucket-1, my_bucket, my_bucket+1]:
            if b in theirs_buckets:
                candidates.extend(theirs_buckets[b])
        
        if not candidates: 
            # å¦‚æœæ²¡æ‰¾åˆ°ç›¸è¿‘é•¿åº¦çš„ï¼Œåˆ™é€€å›åˆ°å…¨é‡æœç´¢
            candidates = theirs_records
            
        my_char_set = set(my_quote)
        
        for their_row in candidates:
            their_quote = str(their_row.get('quote', ''))
            
            if not my_quote and not their_quote:
                ratio = 1.0
            else:
                # ä¼˜åŒ–ï¼šé€šè¿‡ Jaccard ç›¸ä¼¼åº¦å¿«é€Ÿæ’é™¤æ˜æ˜¾ä¸åŒ¹é…çš„
                their_char_set = set(their_quote)
                intersection = len(my_char_set & their_char_set)
                union = len(my_char_set | their_char_set)
                jaccard = intersection / union if union > 0 else 0
                
                if jaccard < 0.3: 
                    continue
                    
                ratio = SequenceMatcher(None, my_quote, their_quote).ratio()
            
            if ratio > best_ratio:
                best_ratio = ratio
                best_match = their_row
        
        status = "unique"
        their_code = None
        if best_ratio >= match_threshold:
            their_code = str(best_match.get('code', ''))
            # æ— è®ºæ˜¯å¦è¾¾åˆ° match_thresholdï¼Œåªè¦æ‰¾åˆ°æœ€ä½³åŒ¹é…ï¼Œå°±è®°å½•å…¶ç›¸ä¼¼åº¦å’Œé˜Ÿå‹ä»£ç 
            if my_code.strip() == their_code.strip(): status = "agreed"
            else: status = "conflict"
        
        alignment.append({
            "quote": my_quote, "my_code": my_code, "their_code": their_code,
            "status": status, "similarity": best_ratio,
            "raw_row_idx": my_row.get('original_row_index')
        })
    return alignment

def display_merge_groups(df, groups_to_display, mode_key):
    """å°è£…æ ‡ç­¾åˆå¹¶çš„äº¤äº’æ˜¾ç¤ºé€»è¾‘"""
    if not groups_to_display:
        st.success("æš‚æ— å»ºè®®æˆ–å·²å¤„ç†å®Œæ¯•ã€‚")
        return

    # ä»…å¯¹è¦å±•ç¤ºçš„ groups è¿›è¡Œæ’åº
    sorted_groups = sorted(groups_to_display.items(), key=lambda x: x[1]['score'], reverse=True)
    
    for gid, data in sorted_groups:
        codes = data["codes"]
        with st.container(border=True):
            col_info, col_act = st.columns([3, 1])
            with col_info:
                st.write(f"**å»ºè®®ç»„** (ç›¸ä¼¼åº¦ {data['score']:.2f})")
                
                # Check if code is still in the main df before listing
                current_active_codes = st.session_state.open_codes['code'].dropna().unique().tolist()
                default_codes = [c for c in codes if c in current_active_codes]
                all_codes_options = sorted(list(set(current_active_codes + codes)))
                
                keep = st.multiselect(
                    "åŒ…å«æ ‡ç­¾", 
                    all_codes_options, 
                    default=default_codes, 
                    key=f"ms_{gid}_{mode_key}"
                )
                
                if keep:
                    with st.expander("ğŸ“„ æŸ¥çœ‹å¼•æ–‡", expanded=False):
                        filtered_df = df[df['code'].isin(keep)][['code', 'quote']]
                        n_sample = min(5, len(filtered_df))
                        if n_sample > 0:
                            sub = filtered_df.sample(n_sample)
                            st.dataframe(sub, width="stretch", hide_index=True)
            with col_act:
                freqs = df[df['code'].isin(keep)]['code'].value_counts()
                rec_name = freqs.idxmax() if not freqs.empty else ""
                new_n = st.text_input("åˆå¹¶ä¸º", value=rec_name, key=f"nn_{gid}_{mode_key}")
                if st.button("âœ… åˆå¹¶", key=f"bm_{gid}_{mode_key}"):
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å˜åŒ–
                    if new_n and new_n not in keep and keep:
                        push_history(f"åˆå¹¶æ ‡ç­¾: {keep} -> {new_n}") # Undo
                        # ç¡®ä¿åªæ›¿æ¢åœ¨ keep åˆ—è¡¨ä¸”åœ¨å½“å‰ df ä¸­çš„ code
                        replace_codes = [c for c in keep if c in st.session_state.open_codes['code'].values]
                        st.session_state.open_codes['code'] = st.session_state.open_codes['code'].replace(replace_codes, new_n)
                        save_current_progress(st.session_state.open_codes)
                        
                        # ä» session_state.merge_groups ä¸­åˆ é™¤å·²å¤„ç†çš„ç»„
                        if gid in st.session_state.merge_groups:
                            del st.session_state.merge_groups[gid]
                            
                        st.success("å·²åˆå¹¶"); time.sleep(0.5); st.rerun()
                    else:
                        st.warning("æ“ä½œæ— æ•ˆï¼šæ–°æ ‡ç­¾åä¸ºç©ºæˆ–åœ¨æ–°æ ‡ç­¾åå·²ç»åœ¨è¢«åˆå¹¶åˆ—è¡¨ä¸­ã€‚")

# =======================================================================
# 1. é¡µé¢é…ç½®ä¸ CSS
# =======================================================================
st.set_page_config(page_title="åˆ†æå·¥ä½œå°", layout="wide")

st.markdown("""
<style>
    [data-testid="stAppViewContainer"] { background-color: #FDFBF5; }
    .quote-box {
        background-color: #f9f9f9;
        border-left: 4px solid #B0C4DE;
        padding: 10px;
        margin-bottom: 10px;
        font-family: "SimSun", "Times New Roman", serif;
        font-size: 1.1rem;
        line-height: 1.6;
        color: #333;
    }
    .quote-label {
        font-size: 0.8rem;
        color: #888;
        margin-bottom: 4px;
        font-weight: bold;
    }
    .stSortable > div > div {
        background-color: #E6F7FF !important; border: 1px solid #69C0FF !important; color: #003a8c !important; 
        border-radius: 6px !important; font-size: 1.1rem !important; font-weight: 600 !important;
        padding: 10px !important; margin-bottom: 8px !important;
        white-space: normal !important; line-height: 1.4 !important;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05) !important;
    }
    .custom-card-hint {
        background-color: #E6F7FF; border: 1px solid #91D5FF; border-radius: 6px;
        padding: 12px; color: #0050B3; font-size: 1rem; margin-bottom: 15px;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ§© åˆ†æå·¥ä½œå°ï¼šæ¸…æ´—ã€å¯¹é½ä¸å½’ç±»")

# çŠ¶æ€åˆå§‹åŒ–
if 'axial_codes_df' not in st.session_state:
    st.session_state.axial_codes_df = pd.DataFrame(columns=['code', 'category', 'confidence', 'reasoning', 'status'])
if 'clusters_cache' not in st.session_state: st.session_state.clusters_cache = None
if 'history_stack' not in st.session_state: st.session_state.history_stack = []
if 'page_num_align' not in st.session_state: st.session_state.page_num_align = 0


# æ•°æ®åŠ è½½
data_missing = 'open_codes' not in st.session_state or st.session_state.open_codes is None or st.session_state.open_codes.empty
data_invalid = False
if not data_missing:
    if 'code' not in st.session_state.open_codes.columns: data_invalid = True

if data_missing or data_invalid:
    if data_invalid: st.error("âš ï¸ æ•°æ®æ ¼å¼é”™è¯¯ï¼ˆç¼ºå°‘ 'code' åˆ—ï¼‰ã€‚")
    else: st.info("ğŸ‘‹ è¯·ä¸Šä¼ æ•°æ®ä»¥å¼€å§‹åˆ†æã€‚")
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ å¼€æ”¾ç¼–ç ç»“æœè¡¨ (Excel/CSV)", type=['xlsx', 'csv'], key="primary_uploader")
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'): df_load = pd.read_csv(uploaded_file)
            else: df_load = pd.read_excel(uploaded_file)
            if 'code' not in df_load.columns: st.error("æ–‡ä»¶ç¼ºå°‘ 'code' åˆ—ã€‚"); st.stop()
            
            # [INIT] åˆå§‹åŒ–4åˆ—ç»“æ„
            if 'original_row_index' not in df_load.columns:
                df_load['original_row_index'] = range(len(df_load))
                
            if 'original_code' not in df_load.columns: df_load['original_code'] = df_load['code']
            if 'peer_code' not in df_load.columns: df_load['peer_code'] = None
            if 'aligned_code' not in df_load.columns: df_load['aligned_code'] = df_load['code']
            if 'quote' not in df_load.columns: df_load['quote'] = df_load['code'] # å‡å®š quote ç¼ºå¤±æ—¶ï¼Œç”¨ code æ›¿ä»£
            
            st.session_state.open_codes = df_load
            reset_analysis_state() 
            st.success("âœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
            time.sleep(1); st.rerun()
        except Exception as e: st.error(f"è¯»å–å¤±è´¥: {e}"); st.stop()
    else: st.stop()

# ç¡®ä¿åˆ—å­˜åœ¨ (é˜²æ­¢ç”¨æˆ·è·³è¿‡åˆå§‹åŒ–æ­¥éª¤)
df = st.session_state.open_codes
for col in ['original_code', 'peer_code', 'aligned_code', 'original_row_index', 'quote']:
    if col not in df.columns:
        if col == 'peer_code': df[col] = None
        elif col == 'original_row_index': df[col] = range(len(df))
        elif col == 'quote': df[col] = df['code']
        else: df[col] = df['code']
st.session_state.open_codes = df 

unique_codes = df['code'].dropna().unique().tolist()

# ä¾§è¾¹æ ä¸API
with st.sidebar:
    st.header("ğŸ“‚ è¿›åº¦ç®¡ç†")
    
    # [NEW] æ’¤é”€æŒ‰é’®
    if st.session_state.history_stack:
        if st.button(f"â†©ï¸ æ’¤é”€ä¸Šä¸€æ­¥ ({len(st.session_state.history_stack)})", type="primary", width="stretch"):
            perform_undo()
    else:
        st.button("â†©ï¸ æ’¤é”€ (æ— è®°å½•)", disabled=True, width="stretch")
    
    st.divider()
    
    if os.path.exists(RECOVERY_DIR):
        jsonl_files = glob.glob(os.path.join(RECOVERY_DIR, "*.jsonl"))
        jsonl_files.sort(key=os.path.getmtime, reverse=True)
        if jsonl_files:
            with st.expander("ğŸ“¥ åŠ è½½å†å²è¿›åº¦", expanded=True):
                selected_file = st.selectbox("é€‰æ‹©å†å²æ–‡ä»¶", [os.path.basename(f) for f in jsonl_files], index=0)
                if st.button("ğŸ”„ è½½å…¥é€‰ä¸­æ–‡ä»¶", width="stretch"):
                    filepath = os.path.join(RECOVERY_DIR, selected_file)
                    df_loaded = load_from_jsonl(filepath)
                    if not df_loaded.empty:
                        st.session_state.open_codes = df_loaded
                        reset_analysis_state() 
                        st.success(f"æˆåŠŸè½½å…¥ï¼")
                        time.sleep(1); st.rerun()
    
    st.divider()
    with st.expander("ğŸ”‘ API Key è®¾ç½®", expanded=not bool(st.session_state.get('api_key'))):
        val = st.session_state.get('api_key', '')
        new_key = st.text_input("DashScope Key", value=val, type="password")
        if new_key != val:
            st.session_state.api_key = new_key
            st.success("Key å·²æ›´æ–°")
            
api_ready = bool(st.session_state.get('api_key'))

# =======================================================================
# 2. ä¸»é€‰é¡¹å¡å¸ƒå±€
# =======================================================================
tab_align, tab_clean, tab_kanban = st.tabs(["ğŸ¤ é˜Ÿå‹å¯¹é½ (åˆ†æ­§è§£å†³)", "ğŸ§¹ æ ‡ç­¾æ¸…æ´— (åŒä¹‰åˆå¹¶)", "ğŸ§± ç§¯æœ¨å½’ç±» (è½´å¿ƒåˆ†æ)"])

# -----------------------------------------------------------------------
# TAB 1: é˜Ÿå‹å¯¹é½
# -----------------------------------------------------------------------
with tab_align:
    st.caption("ä¸Šä¼ é˜Ÿå‹çš„ç¼–ç æ–‡ä»¶ï¼ŒAIå°†è‡ªåŠ¨å¯¹é½å¹¶åˆ—å‡ºå·®å¼‚ã€‚")
    file_peer = st.file_uploader("ä¸Šä¼ é˜Ÿå‹æ–‡ä»¶", type=['xlsx', 'csv'])
    
    if file_peer:
        try:
            if file_peer.name.endswith('.csv'): df_peer = pd.read_csv(file_peer)
            else: df_peer = pd.read_excel(file_peer)
            
            # ç¡®ä¿ df_peer æœ‰ quote å’Œ code åˆ—
            if 'quote' not in df_peer.columns: 
                 st.warning("é˜Ÿå‹æ–‡ä»¶ç¼ºå°‘ 'quote' åˆ—ï¼Œæ— æ³•å¯¹é½ã€‚è¯·ç¡®ä¿æ–‡ä»¶ç»“æ„å®Œæ•´ã€‚")
                 file_peer = None
                 st.stop()
            if 'code' not in df_peer.columns: df_peer['code'] = df_peer['quote'] # å‡è®¾ code ç¼ºå¤±æ—¶ç”¨ quote ä»£æ›¿
            
            # é‡æ–°æ¯”å¯¹æŒ‰é’®ï¼Œç¡®ä¿è®¡ç®—ç»“æœæ˜¯æœ€æ–°çš„
            if 'alignment_results' not in st.session_state or st.button("ğŸ”„ é‡æ–°å¯¹æ¯”", key="re_align_btn"):
                with st.spinner("æ­£åœ¨å¿«é€Ÿæ¯”å¯¹..."):
                    # ä½¿ç”¨ç¨å¾®ä½çš„é˜ˆå€¼ç¡®ä¿æ‰€æœ‰æ½œåœ¨åŒ¹é…éƒ½è¢«æ•è·
                    results = align_records_by_quote(df, df_peer, match_threshold=0.6)
                    st.session_state.alignment_results = results
                    
                    # å¼ºåŠ›å›å¡« peer_codeï¼ˆåŒæ­¥é˜Ÿå‹çš„ç¼–ç ï¼‰
                    updates = 0
                    for r in results:
                        if r['raw_row_idx'] is not None and str(r['raw_row_idx']) in df['original_row_index'].astype(str).values:
                             mask = df['original_row_index'].astype(str) == str(r['raw_row_idx'])
                             st.session_state.open_codes.loc[mask, 'peer_code'] = r['their_code']
                             updates += 1
                        elif r['quote']:
                             mask = st.session_state.open_codes['quote'] == r['quote']
                             st.session_state.open_codes.loc[mask, 'peer_code'] = r['their_code']
                             updates += 1
                    
                    if updates > 0:
                        st.toast(f"å·²åŒæ­¥ {updates} æ¡é˜Ÿå‹æ•°æ®")

            # --------------------- æ¨¡å¼é€‰æ‹© ---------------------
            st.divider()
            st.markdown("#### âš™ï¸ é˜Ÿå‹å¯¹é½æ“ä½œæ¨¡å¼")
            align_mode = st.radio(
                "é€‰æ‹©è‡ªåŠ¨åŒ–ç¨‹åº¦ï¼š",
                ["åŠè‡ªåŠ¨ (æ‰‹åŠ¨ç¡®è®¤)", "åˆ†æ®µæ¨¡å¼ (éƒ¨åˆ†è‡ªåŠ¨)", "è‡ªåŠ¨ (å…¨éƒ¨é‡‡çº³)"],
                horizontal=True,
                key="align_mode_select"
            )
            st.divider()
            
            results = st.session_state.alignment_results
            conflicts_all = [r for r in results if r['status'] == 'conflict']
            conflicts = [] # [FIX] Initialize variable to avoid NameError in 'Automatic' mode

            # --------------------- è‡ªåŠ¨æ¨¡å¼ ---------------------
            if align_mode == "è‡ªåŠ¨ (å…¨éƒ¨é‡‡çº³)":
                if st.button("ğŸš€ è¿è¡Œå…¨è‡ªåŠ¨å¯¹é½/é‡‡çº³", type="primary", key="auto_align_run"):
                    push_history("å…¨è‡ªåŠ¨é˜Ÿå‹å¯¹é½")
                    total_aligned = 0
                    
                    # è‡ªåŠ¨é‡‡çº³é€»è¾‘ï¼šæ‰€æœ‰ç›¸ä¼¼åº¦ >= 0.6 çš„åˆ†æ­§ï¼Œé»˜è®¤é‡‡çº³ã€é˜Ÿå‹ã€‘çš„ç¼–ç ï¼ˆå› ä¸ºå¯¹é½çš„ç›®çš„æ˜¯ç»Ÿä¸€ï¼‰
                    for item in conflicts_all:
                        if item['similarity'] >= 0.6: 
                            target_code = item['their_code'] 
                            
                            # æ›´æ–° open_codes ä¸­çš„ code å’Œ aligned_code
                            mask = (st.session_state.open_codes['quote'] == item['quote']) & \
                                   (st.session_state.open_codes['original_code'] == item['my_code'])

                            if mask.any():
                                st.session_state.open_codes.loc[mask, 'aligned_code'] = target_code
                                st.session_state.open_codes.loc[mask, 'code'] = target_code
                                item['status'] = 'auto_resolved'
                                total_aligned += 1
                    
                    save_current_progress(st.session_state.open_codes)
                    st.success(f"ğŸ‰ è¿è¡Œç»“æŸï¼Œå·²è‡ªåŠ¨å¤„ç† {total_aligned} å¤„åˆ†æ­§ã€‚")
                    st.session_state.alignment_results = results # æ›´æ–°çŠ¶æ€
                    st.rerun()
                
                st.info("âš ï¸ **æ³¨æ„ï¼š** æ­¤æ¨¡å¼å°†å¯¹æ‰€æœ‰å¼•æ–‡ç›¸ä¼¼åº¦ $\ge 0.6$ çš„åˆ†æ­§è¿›è¡Œè‡ªåŠ¨å†³ç­–ï¼ˆé‡‡çº³é˜Ÿå‹ç¼–ç ï¼‰ã€‚")

            # --------------------- åˆ†æ®µæ¨¡å¼ ---------------------
            elif align_mode == "åˆ†æ®µæ¨¡å¼ (éƒ¨åˆ†è‡ªåŠ¨)":
                st.markdown("##### åˆ†æ®µæ¨¡å¼ç•Œé™è®¾ç½®")
                col_a, col_m = st.columns(2)
                threshold_auto = col_a.slider("è‡ªåŠ¨é‡‡çº³é˜ˆå€¼ ($\geq$):", 0.7, 1.0, 0.95, key="align_auto_thresh")
                threshold_manual = col_m.slider("äººå·¥å¤æ ¸é˜ˆå€¼ (ä»‹äº):", 0.0, threshold_auto, 0.60, key="align_manual_thresh")
                
                if st.button("ğŸš€ è¿è¡Œåˆ†æ®µå¤„ç† (è‡ªåŠ¨é‡‡çº³é«˜ç½®ä¿¡åŒº)", type="primary", key="segment_align_run"):
                    push_history("åˆ†æ®µæ¨¡å¼é˜Ÿå‹å¯¹é½")
                    auto_resolved_count = 0
                    
                    # A. å¤„ç†é«˜ç½®ä¿¡åº¦åŒºåŸŸï¼ˆè‡ªåŠ¨é€šè¿‡ï¼‰
                    for item in conflicts_all:
                        if item['status'] == 'conflict' and item['similarity'] >= threshold_auto:
                            target_code = item['their_code']
                            
                            mask = (st.session_state.open_codes['quote'] == item['quote']) & \
                                   (st.session_state.open_codes['original_code'] == item['my_code'])

                            if mask.any():
                                st.session_state.open_codes.loc[mask, 'aligned_code'] = target_code
                                st.session_state.open_codes.loc[mask, 'code'] = target_code
                                item['status'] = 'auto_resolved'
                                auto_resolved_count += 1
                    
                    save_current_progress(st.session_state.open_codes)
                    st.success(f"ğŸ‰ é«˜ç½®ä¿¡åº¦åŒºåŸŸå·²å¤„ç†ï¼å·²è‡ªåŠ¨é‡‡çº³ {auto_resolved_count} æ¡è®°å½•ã€‚")
                    st.session_state.alignment_results = results
                    st.rerun()

                # B. äººå·¥å¤æ ¸åŒºåŸŸï¼šæ˜¾ç¤ºç»™ç”¨æˆ·äº¤äº’ (similarity ä»‹äºä¸¤ä¸ªé˜ˆå€¼ä¹‹é—´)
                conflicts = [r for r in conflicts_all if r['similarity'] >= threshold_manual and r['similarity'] < threshold_auto]
                
                if conflicts:
                    st.warning(f"ğŸ¤– AI ä¸ç¡®å®šåŒºåŸŸï¼šä»éœ€äººå·¥å¤æ ¸ {len(conflicts)} å¤„åˆ†æ­§")
                else:
                    st.success("ğŸ‰ åˆ†æ®µæ¨¡å¼ä¸‹ï¼Œäººå·¥å¤æ ¸åŒºåŸŸå·²æ¸…ç©ºï¼")
                    
            # --------------------- åŠè‡ªåŠ¨æ¨¡å¼ (æ‰‹åŠ¨ç¡®è®¤) ---------------------
            else: # align_mode == "åŠè‡ªåŠ¨ (æ‰‹åŠ¨ç¡®è®¤)"
                conflicts = conflicts_all
                if conflicts:
                    st.warning(f"ğŸ“¢ å‘ç° {len(conflicts)} å¤„åˆ†æ­§ï¼Œè¯·æ‰‹åŠ¨å¤æ ¸ï¼š")
                else:
                    st.success("ğŸ‰ æ‰€æœ‰åˆ†æ­§å·²è§£å†³ï¼")
            
            # --------------------- äº¤äº’å±•ç¤ºé€»è¾‘ï¼ˆåº”ç”¨äºåˆ†æ®µ/åŠè‡ªåŠ¨æ¨¡å¼ä¸‹çš„ conflictsï¼‰ ---------------------
            
            if conflicts:
                page_size = 4
                start_idx = st.session_state.page_num_align * page_size
                current_batch = conflicts[start_idx:start_idx+page_size]
                
                st.progress(min(1.0, (start_idx + len(current_batch)) / len(conflicts)))
                
                for i, item in enumerate(current_batch):
                    idx_real = start_idx + i
                    with st.container(border=True):
                        st.markdown(f"<div class='quote-box'>{item['quote']}</div>", unsafe_allow_html=True)
                        c1, c2 = st.columns(2)
                        with c1:
                            st.info(f"ğŸ‘¤ æˆ‘: **{item['my_code']}** (ç›¸ä¼¼åº¦: {item['similarity']:.2f})")
                            if st.button("ğŸ‘ˆ ä¿ç•™æˆ‘çš„", key=f"k_my_{idx_real}", width="stretch"):
                                item['status'] = 'resolved'; st.rerun()
                        with c2:
                            st.warning(f"ğŸ‘¥ ä»–: **{item['their_code']}**")
                            if st.button("ğŸ‘‰ é‡‡çº³é˜Ÿå‹", key=f"k_th_{idx_real}", width="stretch"):
                                push_history(f"é‡‡çº³é˜Ÿå‹ç¼–ç : {item['their_code']}") # Undo
                                mask = (st.session_state.open_codes['quote'] == item['quote']) & \
                                       (st.session_state.open_codes['original_code'] == item['my_code'])
                                if mask.any():
                                    st.session_state.open_codes.loc[mask, 'aligned_code'] = item['their_code']
                                    st.session_state.open_codes.loc[mask, 'code'] = item['their_code']
                                    save_current_progress(st.session_state.open_codes)
                                    item['status'] = 'resolved'; st.success("å·²æ›´æ–°"); time.sleep(0.5); st.rerun()
                                else: st.error("å®šä½å¤±è´¥")
                        
                        ai_k = f"ai_adv_{idx_real}"
                        custom_code = st.text_input("âœï¸ ä¿®æ”¹ä¸º", value=st.session_state.get(ai_k, item['my_code']), key=f"inp_{idx_real}")
                        ca, cb = st.columns([1, 2])
                        with ca:
                            if st.button("ğŸ¤– é—®AI", key=f"ask_{idx_real}", disabled=not api_ready):
                                prompt = f"å¼•æ–‡ï¼š{item['quote']}\næ ‡ç­¾Aï¼š{item['my_code']}\næ ‡ç­¾Bï¼š{item['their_code']}\nè¯·ç»™å‡ºä¸€ä¸ªæœ€å‡†ç¡®çš„ç®€çŸ­æ ‡ç­¾ï¼š"
                                try:
                                    client = OpenAI(api_key=st.session_state.api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
                                    res = client.chat.completions.create(model="qwen-plus", messages=[{"role":"user","content":prompt}])
                                    st.session_state[ai_k] = res.choices[0].message.content.strip()
                                    st.rerun()
                                except: st.error("API Error")
                        with cb:
                            if st.button("âœ… åº”ç”¨ä¿®æ”¹", key=f"app_{idx_real}", type="primary", width="stretch"):
                                push_history(f"ä¿®æ”¹ç¼–ç ä¸º: {custom_code}") # Undo
                                mask = (st.session_state.open_codes['quote'] == item['quote']) & \
                                       (st.session_state.open_codes['original_code'] == item['my_code'])
                                if mask.any():
                                    st.session_state.open_codes.loc[mask, 'aligned_code'] = custom_code
                                    st.session_state.open_codes.loc[mask, 'code'] = custom_code
                                    save_current_progress(st.session_state.open_codes)
                                    item['status'] = 'resolved'; st.success("å·²æ›´æ–°"); time.sleep(0.5); st.rerun()
                                else: st.error("å®šä½å¤±è´¥")

                cp1, cp2 = st.columns(2)
                if st.session_state.page_num_align > 0:
                    if cp1.button("â¬…ï¸ ä¸Šä¸€é¡µ"): st.session_state.page_num_align -= 1; st.rerun()
                if start_idx + page_size < len(conflicts):
                    if cp2.button("ä¸‹ä¸€é¡µ â¡ï¸"): st.session_state.page_num_align += 1; st.rerun()
            
        except Exception as e: st.error(f"Error: {e}")
    
    st.divider()
    if st.button("ğŸ’¾ æ‰‹åŠ¨ä¿å­˜å¯¹é½è¿›åº¦", key="save_align_manual", width="stretch"):
         fn = save_current_progress(st.session_state.open_codes)
         st.success(f"å·²ä¿å­˜")

# -----------------------------------------------------------------------
# TAB 2: åŒä¹‰åˆå¹¶
# -----------------------------------------------------------------------
with tab_clean:
    c1, c2 = st.columns([2, 1])
    c1.markdown("#### ğŸ§¹ æ ‡ç­¾æ ‡å‡†åŒ–"); c1.caption("åˆå¹¶è¯­ä¹‰é‡å¤çš„æ ‡ç­¾ã€‚æ­¤æ“ä½œä»…æ›´æ–°ã€æœ€ç»ˆæ¸…æ´—ç¼–ç ã€‘åˆ—ã€‚")
    # merge_threshold ç”¨äºæ‰«æï¼Œä¸ç”¨äºè‡ªåŠ¨/åˆ†æ®µæ¨¡å¼çš„å†³ç­–
    merge_threshold = c2.slider("æ‰«æç›¸ä¼¼åº¦é˜ˆå€¼", 0.7, 0.99, 0.85, key="scan_thresh_clean_tab2")
    
    if c2.button("ğŸš€ æ‰«æé‡å¤", type="primary"):
        if not api_ready: st.error("éœ€ API Key")
        else:
            with st.spinner("åˆ†æä¸­..."):
                # ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ code åˆ—è¡¨
                u_codes = df['code'].dropna().unique().tolist()
                if len(u_codes) < 2:
                    st.success("æ ‡ç­¾æ•°é‡ä¸è¶³ 2 ä¸ªæˆ–å·²æ— é‡å¤æ ‡ç­¾ï¼Œæ— éœ€æ‰«æã€‚")
                    if 'merge_groups' in st.session_state: del st.session_state.merge_groups
                else:
                    embs = get_embeddings_dashscope(u_codes, st.session_state.api_key)
                    if len(embs) > 0:
                        # find_synonym_groups å†…éƒ¨ä½¿ç”¨äº† 1-threshold ä½œä¸ºè·ç¦»
                        st.session_state.merge_groups = find_synonym_groups(u_codes, embs, merge_threshold)
                        st.rerun()

    st.divider()
    st.markdown("#### âš™ï¸ æ ‡ç­¾æ¸…æ´—æ“ä½œæ¨¡å¼")
    clean_mode = st.radio(
        "é€‰æ‹©è‡ªåŠ¨åŒ–ç¨‹åº¦ï¼š",
        ["åŠè‡ªåŠ¨ (æ‰‹åŠ¨ç¡®è®¤)", "åˆ†æ®µæ¨¡å¼ (éƒ¨åˆ†è‡ªåŠ¨)", "è‡ªåŠ¨ (å…¨éƒ¨é‡‡çº³)"],
        horizontal=True,
        key="clean_mode_select"
    )
    st.divider()

    # ç¡®ä¿ groups å˜é‡å·²å®šä¹‰å¹¶ä» session_state è·å–
    groups = st.session_state.get('merge_groups', {})
    
    # æ— è®º groups æ˜¯å¦ä¸ºç©ºï¼Œéƒ½å…ˆæ˜¾ç¤ºæç¤º
    if not groups:
        st.info("è¯·å…ˆç‚¹å‡»ä¸Šæ–¹çš„ã€ğŸš€ æ‰«æé‡å¤ã€‘æŒ‰é’®ï¼Œä»¥è·å–ç›¸ä¼¼æ ‡ç­¾å»ºè®®ã€‚")
    
    # --------------------- è‡ªåŠ¨æ¨¡å¼ (é˜ˆå€¼æ˜¾ç¤ºå·²ä¿®å¤) ---------------------
    if clean_mode == "è‡ªåŠ¨ (å…¨éƒ¨é‡‡çº³)":
        st.markdown("##### è‡ªåŠ¨åˆå¹¶è®¾ç½®")
        # [FIX] æ»‘åŠ¨æ¡å·²ç§»è‡³æ­¤å¤„ï¼Œæ— è®º groups æ˜¯å¦ä¸ºç©ºéƒ½æ˜¾ç¤º
        threshold_auto_mode = st.slider(
            "è‡ªåŠ¨åˆå¹¶é˜ˆå€¼ (ç›¸ä¼¼åº¦ $\geq$):", 
            0.7, 1.0, 0.90, 
            key="clean_full_auto_thresh"
        )
        
        if st.button("ğŸš€ è¿è¡Œå…¨è‡ªåŠ¨åˆå¹¶", type="primary", key="auto_clean_run", disabled=not groups):
            if groups:
                push_history("å…¨è‡ªåŠ¨æ ‡ç­¾åˆå¹¶")
                total_merged = 0
                
                groups_to_merge = {k: v for k, v in groups.items() if v['score'] >= threshold_auto_mode}
                
                if not groups_to_merge:
                    st.warning(f"æ²¡æœ‰ç›¸ä¼¼åº¦ $\ge {threshold_auto_mode}$ çš„æ ‡ç­¾ç»„å¯ä¾›åˆå¹¶ã€‚è¯·å°è¯•é™ä½é˜ˆå€¼æˆ–é‡æ–°æ‰«æã€‚")
                else:
                    for gid, data in groups_to_merge.items():
                        codes_to_replace = data["codes"]
                        freqs = df[df['code'].isin(codes_to_replace)]['code'].value_counts()
                        new_n = freqs.idxmax() if not freqs.empty else codes_to_replace[0]
                        
                        st.session_state.open_codes['code'] = st.session_state.open_codes['code'].replace(codes_to_replace, new_n)
                        if gid in st.session_state.merge_groups: del st.session_state.merge_groups[gid]
                        total_merged += len(codes_to_replace)
                    
                    save_current_progress(st.session_state.open_codes)
                    st.session_state.merge_groups = st.session_state.merge_groups
                    st.success(f"ğŸ‰ è¿è¡Œç»“æŸï¼Œå·²è‡ªåŠ¨åˆå¹¶ {total_merged} ä¸ªæ ‡ç­¾ï¼ˆé˜ˆå€¼ $\ge {threshold_auto_mode}$ï¼‰ã€‚")
                    time.sleep(1)
                    st.rerun()

        st.info(f"âš ï¸ **æ³¨æ„ï¼š** æ­¤æ¨¡å¼å°†è‡ªåŠ¨åˆå¹¶æ‰€æœ‰ç›¸ä¼¼åº¦ $\ge {threshold_auto_mode}$ çš„æ ‡ç­¾ç»„ï¼Œæ— éœ€äººå·¥é€ä¸€ç¡®è®¤ã€‚")


    # --------------------- åˆ†æ®µæ¨¡å¼ (é˜ˆå€¼æ˜¾ç¤ºå·²ä¿®å¤) ---------------------
    elif clean_mode == "åˆ†æ®µæ¨¡å¼ (éƒ¨åˆ†è‡ªåŠ¨)":
        
        st.markdown("##### åˆ†æ®µæ¨¡å¼ç•Œé™è®¾ç½®")
        col_a, col_m = st.columns(2)
        # [FIX] æ»‘åŠ¨æ¡å·²ç§»è‡³æ­¤å¤„ï¼Œæ— è®º groups æ˜¯å¦ä¸ºç©ºéƒ½æ˜¾ç¤º
        threshold_auto = col_a.slider("è‡ªåŠ¨åˆå¹¶é˜ˆå€¼ ($\geq$):", 0.85, 1.0, 0.90, key="clean_auto_thresh")
        threshold_manual = col_m.slider("äººå·¥å¤æ ¸é˜ˆå€¼ (ä»‹äº):", 0.70, threshold_auto, 0.80, key="clean_manual_thresh")
        
        if st.button("ğŸš€ è¿è¡Œåˆ†æ®µå¤„ç† (è‡ªåŠ¨åˆå¹¶é«˜ç½®ä¿¡åŒº)", type="primary", key="segment_clean_run", disabled=not groups):
            if groups:
                push_history("åˆ†æ®µæ¨¡å¼æ ‡ç­¾åˆå¹¶")
                auto_merged_count = 0
                
                # A. æ‰¾å‡ºè¦è‡ªåŠ¨å¤„ç†çš„ç»„
                high_conf_groups = {k: v for k, v in groups.items() if v['score'] >= threshold_auto}
                
                if not high_conf_groups:
                    st.warning(f"æœ¬æ¬¡è¿è¡Œæ²¡æœ‰å‘ç°ç›¸ä¼¼åº¦ $\ge {threshold_auto}$ çš„æ ‡ç­¾ç»„ã€‚")
                else:
                    for gid, data in high_conf_groups.items():
                        codes_to_replace = data["codes"]
                        freqs = df[df['code'].isin(codes_to_replace)]['code'].value_counts()
                        new_n = freqs.idxmax() if not freqs.empty else codes_to_replace[0]
                        
                        st.session_state.open_codes['code'] = st.session_state.open_codes['code'].replace(codes_to_replace, new_n)
                        if gid in st.session_state.merge_groups: del st.session_state.merge_groups[gid]
                        auto_merged_count += len(codes_to_replace)
                    
                    save_current_progress(st.session_state.open_codes)
                    st.session_state.merge_groups = st.session_state.merge_groups
                    st.success(f"ğŸ‰ é«˜ç½®ä¿¡åº¦åŒºåŸŸå·²å¤„ç†ï¼å·²è‡ªåŠ¨åˆå¹¶ {auto_merged_count} ä¸ªæ ‡ç­¾ã€‚")
                    time.sleep(0.5)
                st.rerun()

        # B. äººå·¥å¤æ ¸åŒºåŸŸï¼šæ˜¾ç¤ºç»™ç”¨æˆ·äº¤äº’ (similarity ä»‹äºä¸¤ä¸ªé˜ˆå€¼ä¹‹é—´)
        st.divider() # å¢åŠ åˆ†å‰²çº¿
        
        if groups:
            current_groups = st.session_state.get('merge_groups', {})
            manual_conf_groups = {
                k: v for k, v in current_groups.items() 
                if v['score'] >= threshold_manual and v['score'] < threshold_auto
            }

            if manual_conf_groups:
                st.warning(f"ğŸ¤– äººå·¥å¤æ ¸åŒºï¼šç›¸ä¼¼åº¦åœ¨ **[{threshold_manual:.2f}, {threshold_auto:.2f})** ä¹‹é—´ï¼Œå…±éœ€å¤æ ¸ {len(manual_conf_groups)} ç»„å»ºè®®ï¼š")
                display_merge_groups(df, manual_conf_groups, "segment")
            else:
                st.success(f"ğŸ‰ å½“å‰æ— ç›¸ä¼¼åº¦åœ¨ **[{threshold_manual:.2f}, {threshold_auto:.2f})** èŒƒå›´å†…çš„ç»„éœ€è¦äººå·¥å¤æ ¸ï¼")

        
    # --------------------- åŠè‡ªåŠ¨æ¨¡å¼ (ç°æœ‰é€»è¾‘) ---------------------
    else: # clean_mode == "åŠè‡ªåŠ¨ (æ‰‹åŠ¨ç¡®è®¤)"
        if groups:
            st.warning(f"ğŸ“¢ å‘ç° {len(groups)} ç»„å»ºè®®ï¼Œè¯·æ‰‹åŠ¨å¤æ ¸ï¼š")
            # åœ¨åŠè‡ªåŠ¨æ¨¡å¼ä¸‹ï¼Œæ˜¾ç¤ºæ‰€æœ‰ groups
            display_merge_groups(df, groups, "semi_auto")
        
    st.divider()
    if st.button("ğŸ’¾ æ‰‹åŠ¨ä¿å­˜æ¸…æ´—è¿›åº¦", key="save_clean_manual", width="stretch"):
         fn = save_current_progress(st.session_state.open_codes)
         st.success(f"å·²ä¿å­˜")

# -----------------------------------------------------------------------
# TAB 3: ç§¯æœ¨å½’ç±»
# -----------------------------------------------------------------------
with tab_kanban:
    if not HAS_SORTABLE: st.error("éœ€å®‰è£… streamlit-sortables")
    else:
        st.markdown("""<div class="custom-card-hint">ğŸ§± <b>è½´å¿ƒç¼–ç å·¥ä½œå°</b></div>""", unsafe_allow_html=True)
        
        # [NEW] ç§¯æœ¨å·¥ä½œåŒºè¿›åº¦ç®¡ç†
        st.divider()
        st.subheader("ğŸ’¾ ç§¯æœ¨å·¥ä½œåŒºè¿›åº¦")
        
        c_load_save, c_load_dropdown = st.columns([1, 2])
        
        # ä¿å­˜æŒ‰é’®
        with c_load_save:
            save_disabled = 'analysis_df' not in st.session_state or st.session_state.analysis_df.empty
            if st.button("ğŸ’¾ ä¿å­˜å½“å‰ç§¯æœ¨çŠ¶æ€", type="secondary", disabled=save_disabled, key="save_axial_state_manual"):
                if 'analysis_df' in st.session_state and 'sortable_items' in st.session_state:
                    fn = save_analysis_progress(st.session_state.analysis_df, st.session_state.sortable_items)
                    st.success(f"ç§¯æœ¨çŠ¶æ€å·²ä¿å­˜ä¸º {fn}ï¼")
                else:
                    st.warning("æ— æ•°æ®å¯ä¿å­˜ã€‚è¯·å…ˆåˆå§‹åŒ–åˆ†ææ•°æ®ã€‚")

        # è½½å…¥ä¸‹æ‹‰æ¡† (æ”¯æŒä¸Šä¼  JSON/Excel/CSV)
        with c_load_dropdown:
            uploaded_state_file = st.file_uploader(
                "ğŸ“¥ ä¸Šä¼ ç§¯æœ¨çŠ¶æ€æ–‡ä»¶ (.json/.xlsx/.csv)", 
                type=['json', 'xlsx', 'xls', 'csv'], 
                key="uploaded_axial_state"
            )

            if uploaded_state_file is not None:
                if st.button(f"ğŸ”„ è½½å…¥ä¸Šä¼ æ–‡ä»¶: {uploaded_state_file.name}", key="load_uploaded_axial_state", type="primary", use_container_width=True):
                    if load_analysis_progress_from_uploaded_file(uploaded_state_file): 
                        st.success(f"å·²æˆåŠŸè½½å…¥ä¸Šä¼ æ–‡ä»¶ï¼š{uploaded_state_file.name}ã€‚")
                        time.sleep(0.5); st.rerun()
            
            st.markdown("---") # åˆ†éš”ç¬¦
            ensure_dir(ANALYSIS_STATE_DIR)
            analysis_files = glob.glob(os.path.join(ANALYSIS_STATE_DIR, "*.json"))
            analysis_files.sort(key=os.path.getmtime, reverse=True)
            analysis_file_names = [os.path.basename(f) for f in analysis_files]

            if analysis_file_names:
                selected_file = st.selectbox("æˆ–é€‰æ‹©å†å²ç§¯æœ¨çŠ¶æ€ (æœ¬åœ°)", analysis_file_names, index=0)
                if st.button("ğŸ”„ è½½å…¥æœ¬åœ°é€‰ä¸­ç§¯æœ¨çŠ¶æ€", key="load_axial_state_manual_local", type="secondary", use_container_width=True):
                    if load_analysis_progress_from_file(selected_file): 
                        st.success(f"å·²æˆåŠŸè½½å…¥ {selected_file}ã€‚")
                        time.sleep(0.5); st.rerun()
            else:
                st.info("æš‚æ— æœ¬åœ°å†å²ç§¯æœ¨çŠ¶æ€å¯è½½å…¥ã€‚")
        
        st.divider()
        
        # æ¨¡å¼åˆ‡æ¢ - ç§»é™¤æ•£ç‚¹å›¾æ¨¡å¼ (C)
        analysis_mode = st.radio(
            "ğŸ“Š åˆ†ææ¨¡å¼é€‰æ‹©",
            ["æ‹–æ‹½çœ‹æ¿ (å¯å‘å¼)", "Data Editor åˆ†ç»„ (ç¨³å®šç‰ˆ)"],
            index=1, 
            horizontal=True, key="analysis_mode"
        )
        
        cv1, cv2 = st.columns(2)
        with cv1: st.html(generate_html_tag_cloud_color_coded(df))
        with cv2: st.html(generate_html_tag_cloud_color_coded(df)) 
        
        # --- æ¨¡å¼åˆ‡æ¢ä¸‹çš„åˆå§‹åŒ–/é‡ç½®æŒ‰é’® ---
        st.divider()
        if st.session_state.clusters_cache is None or st.button("ğŸ”„ åˆå§‹åŒ–/é‡ç½® åˆ†ææ•°æ®"):
            if not api_ready: st.error("éœ€ API Key")
            else:
                with st.spinner("èšç±»ä¸­..."):
                    uc = df['code'].dropna().unique().tolist()
                    embs = get_embeddings_dashscope(uc, st.session_state.api_key)
                    if len(embs)>0:
                        cl = perform_clustering(uc, embs, distance_threshold=0.4)
                        
                        # Data Editor æ¨¡å¼æ‰€éœ€çš„æ•°æ®å‡†å¤‡ (æ–°å¢ embeddings ç¼“å­˜)
                        cluster_map = {item: lbl for lbl, items in cl.items() for item in items}
                        emb_map = {code: embs[i] for i, code in enumerate(uc)}
                        
                        temp_df = df.copy()
                        temp_df['cluster_id'] = temp_df['code'].apply(lambda x: f"AI Group {cluster_map.get(x, 'NA')}")
                        if 'final_category' not in temp_df.columns:
                            # ä»å·²ä¿å­˜çš„è½´å¿ƒç¼–ç ä¸­åŒæ­¥ç°æœ‰åˆ†ç±»
                            code_to_cat = st.session_state.axial_codes_df.set_index('code')['category'].to_dict()
                            temp_df['final_category'] = temp_df['code'].apply(lambda x: code_to_cat.get(x))

                        unique_code_df = temp_df.drop_duplicates(subset=['code'])
                        unique_code_df = unique_code_df[['code', 'cluster_id', 'final_category']].sort_values(by='cluster_id').reset_index(drop=True)
                        unique_code_df['embedding'] = unique_code_df['code'].apply(lambda x: emb_map.get(x))
                        
                        st.session_state.analysis_df = unique_code_df
                        
                        # çœ‹æ¿æ¨¡å¼æ•°æ®å‡†å¤‡
                        k_data = []
                        leftover = []
                        for lbl, items in cl.items():
                            freqs = {c: len(df[df['code']==c]) for c in items}
                            items_freq = [f"{c} (x{freqs[c]})" for c in items]
                            if len(items)>=2:
                                rep = max(freqs, key=freqs.get)
                                # ç¡®ä¿ä¸ä¸ Data Editor æ¨¡å¼çš„ AI Group æ··æ·†
                                k_data.append({'header': f"AI å»ºè®®ç»„: {rep}", 'items': items_freq}) 
                            else: leftover.extend(items_freq)
                        
                        k_data.insert(0, {'header': 'â“ å¾…å®šåŒº', 'items': leftover})
                        k_data.append({'header': 'ğŸ—‘ï¸ å›æ”¶ç«™', 'items': []})
                        st.session_state.sortable_items = k_data
                        st.session_state.clusters_cache = True
                        st.rerun()

        # --- MODE 1: æ‹–æ‹½çœ‹æ¿ (å¯å‘å¼) ---
        if analysis_mode == "æ‹–æ‹½çœ‹æ¿ (å¯å‘å¼)":
            if 'sortable_items' not in st.session_state:
                st.warning("è¯·å…ˆç‚¹å‡»ã€åˆå§‹åŒ–/é‡ç½® åˆ†ææ•°æ®ã€‘æŒ‰é’®æˆ–ã€è½½å…¥å†å²ç§¯æœ¨çŠ¶æ€ã€‘ã€‚")
            
            else: # å¦‚æœå·²åˆå§‹åŒ–ï¼Œæ˜¾ç¤ºæ‹–æ‹½çœ‹æ¿
                
                with st.expander("ğŸ”§ ç»´åº¦ç®¡ç†", expanded=True):
                    c_m1, c_m2 = st.columns([2, 1])
                    headers = [g['header'] for g in st.session_state.sortable_items]
                    with c_m1:
                        edit_df = pd.DataFrame(headers, columns=["åˆ†ç±»åç§°"])
                        edited_df = st.data_editor(edit_df, width="stretch", hide_index=True, key="hed")
                        if st.button("âœ… åº”ç”¨åç§°ä¿®æ”¹", width="stretch"):
                            push_history("ä¿®æ”¹åˆ†ç±»åç§°") # Undo
                            new_h = edited_df["åˆ†ç±»åç§°"].tolist()
                            if len(new_h) == len(set(new_h)):
                                new_state = []
                                old_map = {g['header']: g['items'] for g in st.session_state.sortable_items}
                                for h in new_h:
                                    # ä¿ç•™åŸæœ‰å†…å®¹
                                    new_state.append({'header': h, 'items': old_map.get(h, [])}) 
                                
                                # å¤„ç†è¢«åˆ é™¤çš„ç»´åº¦ï¼šå°†å†…å®¹ç§»åˆ°å¾…å®šåŒº
                                existing_headers = [g['header'] for g in st.session_state.sortable_items]
                                for old_h, old_i in old_map.items():
                                    if old_h not in new_h and old_h in existing_headers:
                                        # å‡è®¾ 'â“ å¾…å®šåŒº' æ°¸è¿œæ˜¯ç¬¬ä¸€ä¸ª
                                        if new_state and new_state[0]['header'] == 'â“ å¾…å®šåŒº':
                                            new_state[0]['items'].extend(old_i)
                                
                                st.session_state.sortable_items = new_state
                                st.rerun()
                    with c_m2:
                        new_dim = st.text_input("æ–°å»ºç»´åº¦")
                        if st.button("â• æ·»åŠ ", width="stretch"):
                            if new_dim and new_dim not in headers:
                                push_history(f"æ·»åŠ åˆ†ç±»: {new_dim}") # Undo
                                st.session_state.sortable_items.insert(1, {'header': new_dim, 'items': []})
                                st.rerun()

                view_opts = st.multiselect("æ˜¾ç¤ºç»´åº¦", headers, default=headers[:6])
                curr_view = [g for g in st.session_state.sortable_items if g['header'] in view_opts]
                res = sort_items(curr_view, multi_containers=True, direction='vertical', key="kb")
                
                if res != curr_view:
                    push_history("æ‹–æ‹½ç§¯æœ¨åˆ†ç±»") # Undo
                    res_map = {g['header']: g['items'] for g in res}
                    new_full_state = []
                    # é‡æ–°æ„å»ºå®Œæ•´çš„ sortable_items çŠ¶æ€
                    for g in st.session_state.sortable_items:
                        if g['header'] in res_map: g['items'] = res_map[g['header']]
                        new_full_state.append(g)
                    st.session_state.sortable_items = new_full_state
                    st.rerun()

                if st.button("ğŸ’¾ ä¿å­˜å½’ç±»ç»“æœ (è‡³ Page 3)", type="primary", width="stretch"):
                    push_history("ä¿å­˜è½´å¿ƒå½’ç±»") # Undo
                    new_recs = []
                    
                    # ä» sortable_items ä¸­æå–ç»“æœ
                    for g in st.session_state.sortable_items:
                        cat = g['header']
                        # å¿½ç•¥è¾…åŠ©åŒº
                        if 'å›æ”¶' in cat or 'å¾…å®š' in cat or 'AI å»ºè®®ç»„' in cat: continue
                        
                        for it in g['items']:
                            code = it.split(' (x')[0].strip() # æå–æ ‡ç­¾å
                            new_recs.append({
                                'code': code, 'category': cat, 'confidence': 5, 
                                'reasoning': 'äººå·¥æ‹–æ‹½', 'status': 'Accepted'
                            })
                    if new_recs:
                        ndf = pd.DataFrame(new_recs)
                        # æ›´æ–°è½´å¿ƒç¼–ç è¡¨ï¼šä¿ç•™æœªå¤„ç†çš„ï¼Œæ›´æ–°å·²å¤„ç†çš„
                        st.session_state.axial_codes_df = pd.concat([
                            st.session_state.axial_codes_df[~st.session_state.axial_codes_df['code'].isin(ndf['code'])],
                            ndf
                        ], ignore_index=True)
                        st.success(f"å·²ä¿å­˜ {len(new_recs)} æ¡ç»“æœï¼")

        # --- MODE 2: Data Editor åˆ†ç»„ (ç¨³å®šç‰ˆ) ---
        elif analysis_mode == "Data Editor åˆ†ç»„ (ç¨³å®šç‰ˆ)":
            if 'analysis_df' in st.session_state and not st.session_state.analysis_df.empty:
                st.info("ğŸ’¡ **ç¨³å®šæ¨¡å¼ï¼š** æ‹–æ‹½ä¸ç¨³å®šæ—¶ä½¿ç”¨ã€‚è¡¨æ ¼å·²æŒ‰ã€AI å»ºè®®åˆ†ç»„ã€‘æŠ˜å ï¼Œç‚¹å‡»å±•å¼€æŸ¥çœ‹ï¼Œç„¶åæ‰¹é‡è®¾ç½®ã€æœ€ç»ˆå½’ç±»ã€‘ã€‚")
                
                # ç»´åº¦ç®¡ç† for Mode A
                with st.expander("ğŸ”§ ç»´åº¦ç®¡ç† (æ–°å¢è½´å¿ƒåˆ†ç±»)", expanded=True):
                    current_categories = st.session_state.axial_codes_df['category'].dropna().unique().tolist()
                    new_dim = st.text_input("è¾“å…¥æ–°çš„è½´å¿ƒåˆ†ç±»åç§°", key="new_dim_a_input")
                    if st.button("â• æ·»åŠ æ–°åˆ†ç±»", key="add_new_cat_a"):
                        if new_dim and new_dim not in current_categories:
                            # æ·»åŠ ä¸€ä¸ªä¸´æ—¶è®°å½•ï¼Œç¡®ä¿æ–°åˆ†ç±»è¿›å…¥ axial_codes_dfï¼Œä»è€Œå‡ºç°åœ¨ä¸‹æ‹‰èœå•ä¸­
                            new_row = pd.DataFrame([{'code': f'NEW_TEMP_CODE_{int(time.time())}', 'category': new_dim, 'confidence': 0, 'reasoning': 'User Added', 'status': 'Pending'}])
                            st.session_state.axial_codes_df = pd.concat([st.session_state.axial_codes_df, new_row], ignore_index=True)
                            st.success(f"å·²æ·»åŠ åˆ†ç±»ï¼š{new_dim}")
                            time.sleep(0.5)
                            st.rerun() # åˆ·æ–°ä»¥æ›´æ–°ä¸‹æ‹‰æ¡†é€‰é¡¹

                # Get all categories including newly added ones
                current_categories = st.session_state.axial_codes_df['category'].dropna().unique().tolist()
                
                # æ’é™¤ embedding åˆ—
                df_to_edit = st.session_state.analysis_df.drop(columns=['embedding'], errors='ignore')
                
                edited_analysis_df = st.data_editor(
                    df_to_edit,
                    column_config={
                        "code": "å¼€æ”¾ç¼–ç ",
                        "cluster_id": st.column_config.TextColumn("AI å»ºè®®åˆ†ç»„", disabled=True),
                        "final_category": st.column_config.SelectboxColumn(
                            "âœ… æœ€ç»ˆå½’ç±»",
                            options=["(æœªå½’ç±»)"] + current_categories, # é€‰é¡¹åŒ…æ‹¬æ‰€æœ‰å·²åˆ›å»ºçš„åˆ†ç±»å’Œæœªå½’ç±»
                            required=True
                        )
                    },
                    hide_index=True,
                    num_rows="dynamic",
                    column_order=("cluster_id", "code", "final_category"),
                    key="data_editor_mode",
                    use_container_width=True
                )

                if st.button("ğŸ’¾ ä¿å­˜ Data Editor å½’ç±»ç»“æœ", type="primary", width="stretch"):
                    push_history("ä¿å­˜ Data Editor è½´å¿ƒå½’ç±»") # Undo
                    new_recs = []
                    # ç¡®ä¿å¤„ç† edited_analysis_df å¾—åˆ°çš„ç»“æœ
                    for _, row in edited_analysis_df.iterrows():
                        cat = row['final_category']
                        code = row['code']
                        if cat and cat != "(æœªå½’ç±»)":
                             new_recs.append({
                                'code': code, 'category': cat, 'confidence': 5, 
                                'reasoning': 'Data Editor åˆ†ç»„ç¡®è®¤', 'status': 'Accepted'
                            })
                    
                    if new_recs:
                        ndf = pd.DataFrame(new_recs)
                        # æ›´æ–°è½´å¿ƒç¼–ç è¡¨ï¼ŒåŒæ—¶ç§»é™¤ä¸´æ—¶è®°å½•
                        st.session_state.axial_codes_df = pd.concat([
                            st.session_state.axial_codes_df[~st.session_state.axial_codes_df['code'].isin(ndf['code'])],
                            ndf
                        ], ignore_index=True)
                        
                        # æ¸…ç†ä¸´æ—¶ä»£ç 
                        st.session_state.axial_codes_df = st.session_state.axial_codes_df[~st.session_state.axial_codes_df['code'].str.startswith('NEW_TEMP_CODE_')]
                        
                        # æ›´æ–° analysis_df çš„ final_category çŠ¶æ€
                        for code, category in zip(ndf['code'], ndf['category']):
                            st.session_state.analysis_df.loc[st.session_state.analysis_df['code'] == code, 'final_category'] = category
                        
                        st.success(f"å·²ä¿å­˜ {len(new_recs)} æ¡ç»“æœï¼")
            else:
                st.warning("è¯·å…ˆç‚¹å‡»ã€åˆå§‹åŒ–/é‡ç½® åˆ†ææ•°æ®ã€‘æŒ‰é’®æˆ–ã€è½½å…¥å†å²ç§¯æœ¨çŠ¶æ€ã€‘ã€‚")

# =======================================================================
# 5. å…¨é‡æ¸…æ´—æ¸…å• (åŒ…å«æ‰€æœ‰ç‰ˆæœ¬)
# =======================================================================
st.divider()
st.subheader("5ï¸âƒ£ å¼€æ”¾ç¼–ç æ¸…æ´—æ¸…å• (Full Traceability)")
st.caption("å…¨æµç¨‹è¿½æº¯ï¼šåŸå§‹ -> å¯¹é½å -> æœ€ç»ˆæ¸…æ´—ã€‚æ‚¨å¯ä»¥ç›´æ¥åœ¨æ­¤è¡¨æ ¼ä¿®æ”¹ã€æœ€ç»ˆæ¸…æ´—ç¼–ç ã€‘ã€‚")

if not st.session_state.open_codes.empty:
    display_df = st.session_state.open_codes.copy()
    
    col_config = {
        "quote": st.column_config.TextColumn("åŸå§‹å¼•æ–‡", disabled=True, width="medium"),
        "original_code": st.column_config.TextColumn("ğŸ‘¤ æˆ‘çš„åŸå§‹", disabled=True),
        "peer_code": st.column_config.TextColumn("ğŸ‘¥ é˜Ÿå‹åŸå§‹", disabled=True),
        "aligned_code": st.column_config.TextColumn("ğŸ¤ å¯¹é½å (Tab1)", disabled=True),
        "code": st.column_config.TextColumn("âœ… æœ€ç»ˆæ¸…æ´— (Tab2)", disabled=False)
    }
    
    final_cols = ['quote', 'original_code', 'peer_code', 'aligned_code', 'code']
    final_cols = [c for c in final_cols if c in display_df.columns]
    
    edited_clean_df = st.data_editor(
        display_df[final_cols], 
        column_config=col_config,
        use_container_width=True,
        key="clean_editor"
    )
    
    if st.button("ğŸ’¾ ä¿å­˜æ¸…å•ä¿®æ”¹", type="primary", key="save_list_edit"):
        push_history("ä¿®æ”¹æ¸…æ´—æ¸…å•") # Undo
        st.session_state.open_codes['code'] = edited_clean_df['code']
        fn = save_current_progress(st.session_state.open_codes)
        st.success(f"ä¿®æ”¹å·²ä¿å­˜ï¼")
        time.sleep(0.5)
        st.rerun()

# =======================================================================
# 6. ç»“æœå±•ç¤º (è½´å¿ƒç¼–ç )
# =======================================================================
st.divider()
st.subheader("6ï¸âƒ£ å·²ç¡®è®¤çš„è½´å¿ƒç¼–ç ")
if not st.session_state.axial_codes_df.empty:
    # ç§»é™¤ä¸´æ—¶ä»£ç  (å¦‚æœæœ‰)
    display_axial_df = st.session_state.axial_codes_df[~st.session_state.axial_codes_df['code'].str.startswith('NEW_TEMP_CODE_')]
    
    st.dataframe(display_axial_df[['category', 'code']], width="stretch")
